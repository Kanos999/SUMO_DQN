{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4c72361",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sumo-gui'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b7b162f0ec09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msumobin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msumolib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckBinary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sumo-gui'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtraci\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msumobin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtraci\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraci\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVAR_COLLIDING_VEHICLES_IDS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/traci/main.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(cmd, port, numRetries, label, verbose, traceFile, traceGetters, stdout, doSwitch)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Calling \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0msumoProcess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msumoPort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumRetries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"localhost\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msumoProcess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoSwitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceGetters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    856\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    859\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1706\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1707\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sumo-gui'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import traci\n",
    "import sumolib\n",
    "import time\n",
    "\n",
    "environment = \"environments/cross.sumocfg\"\n",
    "sumobin = sumolib.checkBinary('sumo-gui')\n",
    "\n",
    "traci.start([sumobin, '-c', environment])  \n",
    "\n",
    "traci.simulation.subscribe([traci.constants.VAR_COLLIDING_VEHICLES_IDS])\n",
    "\n",
    "# Subscribe to vehicle accelerations for all vehicles\n",
    "for veh_id in traci.vehicle.getIDList():\n",
    "    traci.vehicle.subscribe(veh_id, traci.constants.VAR_ACCELERATION)\n",
    "\n",
    "print(\"Connected to TraCI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sum the delay of all vehicles affected by the traffic light\n",
    "def get_delay(tls_id):\n",
    "    delays = [get_lane_delay(lane_id) for lane_id in traci.trafficlight.getControlledLanes(tls_id)]\n",
    "    return sum(delays)\n",
    "\n",
    "# returns the sum of every vehicle's delay (1 - speed / max_speed) in a given lane\n",
    "def get_lane_delay(lane_id):\n",
    "    max_s = traci.lane.getMaxSpeed(lane_id)\n",
    "    avg_s = traci.lane.getLastStepMeanSpeed(lane_id)\n",
    "    num_veh = traci.lane.getLastStepVehicleNumber(lane_id)\n",
    "    return max(num_veh * (1 - avg_s / max_s), 0) # for some reason this can return small negative values :\\\n",
    "\n",
    "# Function to get the number of vehicles currently waiting\n",
    "def get_waiting_time(tls_id = '0'):\n",
    "    waiting_times = [traci.lane.getLastStepHaltingNumber(lane_id) for lane_id in traci.trafficlight.getControlledLanes(tls_id)]\n",
    "    return sum(waiting_times)\n",
    "\n",
    "# Function that returns the number of emergency stops (acceleration < -4.5m/s^2) caused by the traffic light\n",
    "def num_emergency_stops(tls_id = '0'):\n",
    "    emergency_stops = [get_lane_emergency_stops(lane_id) for lane_id in traci.trafficlight.getControlledLanes(tls_id)]\n",
    "    return sum(emergency_stops)\n",
    "\n",
    "# returns the number of vehicles that had to emergency stop in the last time step (decelerated > 4.5 m/s/s)\n",
    "def get_lane_emergency_stops(lane_id):\n",
    "    emergency_stops = [veh_id for veh_id in traci.lane.getLastStepVehicleIDs(lane_id) if traci.vehicle.getAcceleration(veh_id) < -4.5]\n",
    "    return len(emergency_stops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac27dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.previous_total_delay = 0.0\n",
    "        self.no_action_count = 0\n",
    "        self.previous_phase = None\n",
    "\n",
    "\n",
    "    # Function to reset the SUMO environment\n",
    "    def reset_sumo_environment(self, environment):\n",
    "        traci.load(['-c', environment, '--start', '--step-length', 0.8])\n",
    "        traci.trafficlight.setProgram('0', '0')\n",
    "        \n",
    "        # Get initial state information (modify this based on your state representation)\n",
    "        state = self.get_state()\n",
    "        return state\n",
    "\n",
    "    # Function to step through the SUMO simulation\n",
    "    def step_in_sumo(self, action):\n",
    "        # Apply the action\n",
    "        self.apply_action(action)\n",
    "        \n",
    "        # Step the SUMO simulation forward\n",
    "        traci.simulationStep()\n",
    "        \n",
    "        # Get the new state after taking the action\n",
    "        next_state = self.get_state()\n",
    "        \n",
    "        # Calculate the reward with the specified tls_id\n",
    "        reward = self.calculate_reward(action)\n",
    "        \n",
    "        # Check if the episode is done\n",
    "        done = self.check_done_condition()\n",
    "        \n",
    "        return next_state, reward, done\n",
    "\n",
    "    # Function to get the current state (modify this based on what information you need)\n",
    "    def get_state(self):\n",
    "        # example state, the total delay of each lane, takes into account cars being slowed, or too many cars stopped at a red\n",
    "        state = []\n",
    "        tls_ids = traci.trafficlight.getIDList()\n",
    "        for tls_id in tls_ids:\n",
    "            lanes = traci.trafficlight.getControlledLanes(tls_id)\n",
    "            traffic_light_state = list(map(get_lane_delay, lanes))\n",
    "            state.extend(traffic_light_state)\n",
    "\n",
    "        return np.array(state)\n",
    "\n",
    "    # Function to apply the action (modify based on your action space)\n",
    "    def apply_action(self, action):\n",
    "        # actions changes the phase of the traffic light program (0: do nothing, 1: next phase)\n",
    "        try:\n",
    "            if action == 1:\n",
    "                tls_ids = traci.trafficlight.getIDList()\n",
    "                for tls_id in tls_ids:\n",
    "                    traci.trafficlight.setPhaseDuration(tls_id, 0.4)\n",
    "        except traci.exceptions.FatalTraCIError as e:\n",
    "            print(\"TraCI error:\", e)\n",
    "            traci.close()\n",
    "            return\n",
    "\n",
    "    # Function to calculate the reward (implement your logic)\n",
    "    def calculate_reward(self, action):\n",
    "        collision_weight = -1000.0\n",
    "        emergency_braking_weight = -80.0\n",
    "        delay_change_weight = 4.0\n",
    "        consecutive_action_weight = 0.1\n",
    "        # Define the emergency braking threshold (e.g., -4.5 m/sÂ²)\n",
    "        EMERGENCY_BRAKE_THRESHOLD = -1.0\n",
    "\n",
    "        # Check for any collisions\n",
    "        collision_data = traci.simulation.getSubscriptionResults()\n",
    "        collision = 0\n",
    "        if collision_data and traci.constants.VAR_COLLIDING_VEHICLES_IDS in collision_data:\n",
    "            collision = 1\n",
    "        \n",
    "        # Check for emergency braking in each vehicle\n",
    "        # current_vehicles = traci.vehicle.getIDList()\n",
    "        # num_emergency_brakes = 0\n",
    "        # for veh_id in current_vehicles:\n",
    "        #     if traci.vehicle.getSubscriptionResults(veh_id) is None:\n",
    "        #         # Subscribe to acceleration if not already subscribed\n",
    "        #         traci.vehicle.subscribe(veh_id, traci.constants.VAR_ACCELERATION)\n",
    "\n",
    "        #     # Debug: print the subscription data for each vehicle\n",
    "        #     subscription_data = traci.vehicle.getSubscriptionResults(veh_id)\n",
    "\n",
    "        #     # Get the subscribed acceleration data for each vehicle\n",
    "        #     acceleration = subscription_data.get(traci.constants.VAR_ACCELERATION, None)\n",
    "        #     print(\"accel:\", acceleration)\n",
    "        #     # Check if acceleration is available and meets the emergency brake threshold\n",
    "        #     if acceleration is not None and acceleration < EMERGENCY_BRAKE_THRESHOLD:\n",
    "        #         num_emergency_brakes += 1\n",
    "\n",
    "        # Reward for consecutive non-actions\n",
    "        IDEAL_TIMESTEPS = 100\n",
    "        if action == 1:\n",
    "            self.no_action_count = 0\n",
    "        else:\n",
    "            self.no_action_count += 1\n",
    "\n",
    "\n",
    "        # Calculate the change in delay from previous\n",
    "        tls_ids = traci.trafficlight.getIDList()\n",
    "        current_total_delay = 0\n",
    "        for tls_id in tls_ids:\n",
    "            # Get lanes controlled by this traffic light system\n",
    "            controlled_lanes = traci.trafficlight.getControlledLanes(tls_id)\n",
    "            \n",
    "            # Sum the waiting times of all vehicles on the approaching lanes\n",
    "            for lane_id in controlled_lanes:\n",
    "                current_total_delay += traci.lane.getWaitingTime(lane_id)\n",
    "    \n",
    "\n",
    "\n",
    "        reward = delay_change_weight * (self.previous_total_delay - current_total_delay) \\\n",
    "                + collision_weight * collision \\\n",
    "                + consecutive_action_weight * self.no_action_count\n",
    "                # + emergency_braking_weight * num_emergency_brakes\n",
    "\n",
    "\n",
    "        # print(f\"delay: {(delay_change_weight * (self.previous_total_delay - current_total_delay)):.2f}, \\\n",
    "        #         collision: {collision_weight * collision}, \\\n",
    "        #         reward: {reward}\")\n",
    "        \n",
    "        self.previous_total_delay = current_total_delay\n",
    "        \n",
    "        return reward\n",
    "\n",
    "\n",
    "    # Function to check if the simulation should terminate\n",
    "    def check_done_condition(self):\n",
    "        # Example condition: terminate if simulation time exceeds a limit\n",
    "        collision_data = traci.simulation.getSubscriptionResults()\n",
    "        \n",
    "        # Check for any collisions\n",
    "        if collision_data and traci.constants.VAR_COLLIDING_VEHICLES_IDS in collision_data:\n",
    "            return True\n",
    "        \n",
    "        current_time = traci.simulation.getTime()\n",
    "        return current_time > 800  # Change this threshold as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd2c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network for the Q-function\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_state_params, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_state_params, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20726f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RL agent\n",
    "class RLAgent:\n",
    "    def __init__(self, n_state_params, n_actions):\n",
    "        self.n_state_params = n_state_params\n",
    "        self.n_actions = n_actions\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 0.2  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = DQN(n_state_params, n_actions)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        state = torch.FloatTensor(state)\n",
    "        q_values = self.model(state)\n",
    "        return np.argmax(q_values.detach().numpy())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model(torch.FloatTensor(next_state)).detach().numpy())\n",
    "            target_f = self.model(torch.FloatTensor(state)).detach().numpy()\n",
    "            # Check if action index is valid\n",
    "            if 0 <= action < self.n_actions:\n",
    "                target_f[action] = target\n",
    "            else:\n",
    "                print(f\"Invalid action: {action}\")\n",
    "\n",
    "            # Convert back to tensor for loss calculation\n",
    "            target_f_tensor = torch.FloatTensor(target_f)\n",
    "            self.model.zero_grad()\n",
    "            loss = self.criterion(target_f_tensor, self.model(torch.FloatTensor(state)))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f581da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation interaction loop\n",
    "def run_simulation(agent, env, num_episodes, batch_size):\n",
    "    for e in range(num_episodes):\n",
    "        state = env.reset_sumo_environment(environment)  # Reset the SUMO environment and get the initial state\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step_in_sumo(action)  # Step through the SUMO simulation\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"Episode: {e+1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "        agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cecb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inputs: 12\n",
      "actions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaneh\\AppData\\Local\\Temp\\ipykernel_18912\\248770233.py:6: UserWarning: Call to deprecated function getCompleteRedYellowGreenDefinition, use getAllProgramLogics instead.\n",
      "  program = traci.trafficlight.getCompleteRedYellowGreenDefinition('0')[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/100, Total Reward: -5231.799999999999\n",
      "Episode: 2/100, Total Reward: 5295.09999999999\n",
      "Episode: 3/100, Total Reward: 1873.4\n",
      "Episode: 4/100, Total Reward: -394.90000000000265\n",
      "Episode: 5/100, Total Reward: 1387.4999999999977\n",
      "Episode: 6/100, Total Reward: 624.0000000000006\n",
      "Episode: 7/100, Total Reward: 1217.7999999999959\n",
      "Episode: 8/100, Total Reward: 1856.799999999997\n",
      "Episode: 9/100, Total Reward: 323.9000000000003\n",
      "Episode: 10/100, Total Reward: -572.9999999999986\n",
      "Episode: 11/100, Total Reward: 1841.0999999999967\n",
      "Episode: 12/100, Total Reward: -2186.400000000001\n",
      "Episode: 13/100, Total Reward: 2265.9999999999955\n",
      "Episode: 14/100, Total Reward: 172.69999999999942\n",
      "Episode: 15/100, Total Reward: -1342.5999999999995\n",
      "Episode: 16/100, Total Reward: 1991.5000000000016\n",
      "Episode: 17/100, Total Reward: -176.49999999999991\n",
      "Episode: 18/100, Total Reward: -413.30000000000075\n",
      "Episode: 19/100, Total Reward: 567.4999999999994\n",
      "Episode: 20/100, Total Reward: -1435.3000000000006\n",
      "Episode: 21/100, Total Reward: 1255.7000000000003\n",
      "Episode: 22/100, Total Reward: 231.09999999999866\n",
      "Episode: 23/100, Total Reward: -62.80000000000048\n",
      "Episode: 24/100, Total Reward: 95.30000000000008\n",
      "Episode: 25/100, Total Reward: -1835.800000000001\n",
      "Episode: 26/100, Total Reward: 1848.8000000000004\n",
      "Episode: 27/100, Total Reward: -227.50000000000014\n",
      "Episode: 28/100, Total Reward: -301.20000000000005\n",
      "Episode: 29/100, Total Reward: 551.6000000000005\n",
      "Episode: 30/100, Total Reward: -573.6000000000003\n",
      "Episode: 31/100, Total Reward: 565.6999999999995\n",
      "Episode: 32/100, Total Reward: -865.7000000000007\n",
      "Episode: 33/100, Total Reward: 829.2000000000002\n",
      "Episode: 34/100, Total Reward: 153.09999999999994\n",
      "Episode: 35/100, Total Reward: -3940.000000000001\n",
      "Episode: 36/100, Total Reward: 3330.400000000001\n",
      "Episode: 37/100, Total Reward: -768.9999999999986\n",
      "Episode: 38/100, Total Reward: 1388.199999999999\n",
      "Episode: 39/100, Total Reward: -21.89999999999931\n",
      "Episode: 40/100, Total Reward: -48.60000000000028\n",
      "Episode: 41/100, Total Reward: 4.200000000000166\n",
      "Episode: 42/100, Total Reward: -406.5999999999999\n",
      "Episode: 43/100, Total Reward: -603.9999999999993\n",
      "Episode: 44/100, Total Reward: 864.5999999999992\n",
      "Episode: 45/100, Total Reward: 272.7999999999996\n",
      "Episode: 46/100, Total Reward: -165.40000000000097\n",
      "Episode: 47/100, Total Reward: -2500.5000000000005\n",
      "Episode: 48/100, Total Reward: 2524.0999999999963\n",
      "Episode: 49/100, Total Reward: 156.39999999999978\n",
      "Episode: 50/100, Total Reward: -176.40000000000046\n",
      "Episode: 51/100, Total Reward: -400.10000000000036\n",
      "Episode: 52/100, Total Reward: 659.4000000000008\n",
      "Episode: 53/100, Total Reward: -94.29999999999995\n",
      "Episode: 54/100, Total Reward: -4158.8\n",
      "Episode: 55/100, Total Reward: 4318.500000000017\n",
      "Episode: 56/100, Total Reward: -868.6999999999998\n",
      "Episode: 57/100, Total Reward: 548.100000000001\n",
      "Episode: 58/100, Total Reward: 365.4000000000004\n",
      "Episode: 59/100, Total Reward: -193.49999999999804\n",
      "Episode: 60/100, Total Reward: 135.6999999999997\n",
      "Episode: 61/100, Total Reward: -983.8000000000003\n",
      "Episode: 62/100, Total Reward: -878.6000000000033\n",
      "Episode: 63/100, Total Reward: 1934.6999999999998\n",
      "Episode: 64/100, Total Reward: -83.5\n",
      "Episode: 65/100, Total Reward: 32.00000000000045\n",
      "Episode: 66/100, Total Reward: -817.9000000000008\n",
      "Episode: 67/100, Total Reward: 334.50000000000153\n",
      "Episode: 68/100, Total Reward: 544.4999999999992\n",
      "Episode: 69/100, Total Reward: -1121.0000000000002\n",
      "Episode: 70/100, Total Reward: 820.0999999999999\n",
      "Episode: 71/100, Total Reward: 66.60000000000021\n",
      "Episode: 72/100, Total Reward: -623.6000000000009\n",
      "Episode: 73/100, Total Reward: 935.9999999999993\n",
      "Episode: 74/100, Total Reward: -636.4999999999998\n",
      "Episode: 75/100, Total Reward: 646.4000000000007\n",
      "Episode: 76/100, Total Reward: 23.999999999999794\n",
      "Episode: 77/100, Total Reward: -74.00000000000026\n",
      "Episode: 78/100, Total Reward: -1116.7000000000005\n",
      "Episode: 79/100, Total Reward: 1197.4999999999977\n",
      "Episode: 80/100, Total Reward: 16.399999999999768\n",
      "Episode: 81/100, Total Reward: -362.19999999999914\n",
      "Episode: 82/100, Total Reward: -781.1000000000006\n",
      "Episode: 83/100, Total Reward: 1017.9999999999998\n",
      "Episode: 84/100, Total Reward: 48.399999999999636\n",
      "Episode: 85/100, Total Reward: 84.10000000000308\n",
      "Episode: 86/100, Total Reward: -210.60000000000082\n",
      "Episode: 87/100, Total Reward: 89.20000000000039\n",
      "Episode: 88/100, Total Reward: 174.19999999999948\n",
      "Episode: 89/100, Total Reward: -1.6999999999992816\n",
      "Episode: 90/100, Total Reward: -543.2000000000002\n",
      "Episode: 91/100, Total Reward: 202.1000000000011\n",
      "Episode: 92/100, Total Reward: 190.50000000000009\n",
      "Episode: 93/100, Total Reward: 120.20000000000002\n",
      "Episode: 94/100, Total Reward: -872.4000000000002\n",
      "Episode: 95/100, Total Reward: 886.7999999999997\n",
      "Episode: 96/100, Total Reward: -102.50000000000054\n",
      "Episode: 97/100, Total Reward: -1623.6000000000006\n",
      "Episode: 98/100, Total Reward: 941.6000000000008\n",
      "Episode: 99/100, Total Reward: 671.9000000000005\n",
      "Episode: 100/100, Total Reward: -95.90000000000035\n"
     ]
    }
   ],
   "source": [
    "# number of state parameters: parameter for each lane controlled by the traffic light, giving the total delay\n",
    "env = Environment()\n",
    "n_state_params = len(env.get_state())\n",
    "print(\"Number of inputs:\", n_state_params)\n",
    "# Get the full phase program for the traffic light\n",
    "program = traci.trafficlight.getCompleteRedYellowGreenDefinition('0')[0]\n",
    "\n",
    "# Get the number of phases\n",
    "n_actions = 2\n",
    "print(\"actions:\", n_actions)\n",
    "\n",
    "agent = RLAgent(n_state_params, n_actions)\n",
    "run_simulation(agent, env, num_episodes=100, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "67597977df2e21fc2187960fff21482ba330db1716c873fc10e155bad60d32e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
