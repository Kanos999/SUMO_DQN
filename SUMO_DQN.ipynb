{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4c72361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to TraCI\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import traci\n",
    "import sumolib\n",
    "import math\n",
    "\n",
    "environment = \"intersection/sumo_config.sumocfg\"\n",
    "phase_lane_control = np.array([\n",
    "        [\"N2TL_0\", \"N2TL_1\", \"N2TL_2\", \"S2TL_0\", \"S2TL_1\", \"S2TL_2\"],\n",
    "        [\"N2TL_3\", \"S2TL_3\"],\n",
    "        [\"W2TL_0\", \"W2TL_1\", \"W2TL_2\", \"E2TL_0\", \"E2TL_1\", \"E2TL_2\"],\n",
    "        [\"W2TL_3\", \"E2TL_3\"]\n",
    "    ], dtype=object)\n",
    "\n",
    "sumobin = sumolib.checkBinary('sumo-gui')\n",
    "\n",
    "traci.start([sumobin, '-c', environment, '--start'])  \n",
    "\n",
    "traci.simulation.subscribe([traci.constants.VAR_COLLIDING_VEHICLES_IDS])\n",
    "\n",
    "# Subscribe to vehicle accelerations for all vehicles\n",
    "for veh_id in traci.vehicle.getIDList():\n",
    "    traci.vehicle.subscribe(veh_id, traci.constants.VAR_ACCELERATION)\n",
    "\n",
    "# for single agent\n",
    "trafficlight_id = traci.trafficlight.getIDList()[0]\n",
    "controlled_lanes = traci.trafficlight.getControlledLanes(trafficlight_id)\n",
    "TIME_STEP = 0.8 # amount of time (in seconds) per step of the simulation, i.e. 0.01 => 10ms per step\n",
    "\n",
    "print(\"Connected to TraCI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3a366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the number of vehicles currently waiting\n",
    "def get_avg_waiting():\n",
    "    # grouped lanes by shared green light phases, record the number of cars waiting divided by the number of lanes\n",
    "    grouped_avg_waiting = [get_lane_num_waiting(lanes) / len(lanes) for lanes in phase_lane_control]\n",
    "    return grouped_avg_waiting\n",
    "\n",
    "# returns the total number of cars waiting in the set of lanes\n",
    "def get_lane_num_waiting(lanes):\n",
    "    sum = 0\n",
    "    for lane_id in lanes:\n",
    "        sum += traci.lane.getLastStepHaltingNumber(lane_id)\n",
    "    return sum\n",
    "\n",
    "# returns a list of vehicle ids that are currently stopped in one of the lanes\n",
    "def get_waiting_ids(lanes):\n",
    "    ids = []\n",
    "    for lane_id in lanes:\n",
    "        ids.extend([veh_id for veh_id in traci.lane.getLastStepVehicleIDs(lane_id) if traci.vehicle.getSpeed(veh_id) < 0.1])\n",
    "    return np.array(ids)\n",
    "\n",
    "def pct_served(waiting_ids):\n",
    "    if len(waiting_ids) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # vehicles that have been served but exited simulation need to be counted a different way\n",
    "    still_loaded = [veh_id for veh_id in waiting_ids if veh_id in traci.vehicle.getLoadedIDList()]\n",
    "    num_waiting_served = len([veh_id for veh_id in still_loaded if traci.vehicle.getSpeed(veh_id) > 0.5])\n",
    "    num_waiting_served += len(waiting_ids) - len(still_loaded)\n",
    "\n",
    "    return num_waiting_served / len(waiting_ids)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac27dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.prev_action = traci.trafficlight.getPhase(trafficlight_id)\n",
    "        self.yellow_duration = 3 # duration of yellow phases in seconds between actions\n",
    "        self.green_duration = 5 # minimum amount of time the green phases are on for\n",
    "\n",
    "        self.static_action = 0 # adds reward for not changing the phase, prevents flickering\n",
    "        self.waiting_ids = [] # list of vehicle ids that were waiting in one of the lanes now greenlit in the current phase\n",
    "        self.pct_served = 0 # percentage of cars waiting at the relevant lanes that made it through on the last light cycle\n",
    "\n",
    "\n",
    "    # Function to reset the SUMO environment\n",
    "    def reset_sumo_environment(self, environment):\n",
    "        # reload the simulation\n",
    "        traci.load(['-c', environment, '--start', '--step-length', TIME_STEP])\n",
    "        traci.trafficlight.setProgram(trafficlight_id, '0')\n",
    "        \n",
    "        # reset some variables\n",
    "        self.waiting_ids = []\n",
    "        self.pct_served = 0\n",
    "        state = self.get_state()\n",
    "\n",
    "        return state\n",
    "\n",
    "\n",
    "    # Function to step through the SUMO simulation\n",
    "    def step_in_sumo(self, action):\n",
    "        # Apply the action\n",
    "        self.apply_action(action)\n",
    "        \n",
    "        # Step the SUMO simulation forward\n",
    "        traci.simulationStep()\n",
    "        \n",
    "        # Get the new state after taking the action\n",
    "        next_state = self.get_state()\n",
    "        \n",
    "        # Calculate the reward with the specified tls_id\n",
    "        reward = self.calculate_reward()\n",
    "        \n",
    "        # Check if the episode is done\n",
    "        done = self.check_done_condition()\n",
    "        \n",
    "        return next_state, reward, done\n",
    "\n",
    "\n",
    "    # Function to get the current state (modify this based on what information you need)\n",
    "    def get_state(self):\n",
    "        # number of cars in the lanes each phase of the traffic light controls\n",
    "        state = get_avg_waiting()\n",
    "        state.append(self.pct_served) # include the served percent of the current phase\n",
    "        state.append(self.prev_action) # include the current action value\n",
    "        \n",
    "        return np.array(state)\n",
    "\n",
    "\n",
    "    # Function to apply the action (modify based on your action space)\n",
    "    def apply_action(self, action):\n",
    "        if action == self.prev_action:\n",
    "            self.static_action = 1\n",
    "            return\n",
    "        \n",
    "        # simulate the yellow light phase corresponding to the last green phase\n",
    "        self.simulate_phase(2 * self.prev_action + 1, self.yellow_duration)\n",
    "\n",
    "        # get the success parameters of the last light phase\n",
    "        self.pct_served = pct_served(self.waiting_ids)\n",
    "        self.waiting_ids = get_waiting_ids(phase_lane_control[action])\n",
    "        \n",
    "        # change to the new green phase, simulate for the minimum amount of time\n",
    "        self.simulate_phase(2 * action, self.green_duration)\n",
    "        self.prev_action = action\n",
    "\n",
    "\n",
    "    # changes the phase and simulates it for the required amount of time\n",
    "    def simulate_phase(self, action, duration):\n",
    "        traci.trafficlight.setPhase(trafficlight_id, action)\n",
    "        steps = 0\n",
    "        while steps < duration / TIME_STEP:\n",
    "            traci.simulationStep()\n",
    "            steps += 1\n",
    "\n",
    "\n",
    "    # Function to calculate the reward (implement your logic)\n",
    "    def calculate_reward(self):\n",
    "        reward = self.static_action + math.exp(4 * self.pct_served) - math.exp(0.2 * sum(get_avg_waiting()))\n",
    "        \n",
    "        self.static_action = 0\n",
    "        self.pct_served = 0\n",
    "        return reward\n",
    "\n",
    "\n",
    "    # Function to check if the simulation should terminate\n",
    "    def check_done_condition(self):\n",
    "        # Example condition: terminate if simulation time exceeds a limit\n",
    "        collision_data = traci.simulation.getSubscriptionResults()\n",
    "        \n",
    "        # Check for any collisions\n",
    "        if collision_data and traci.constants.VAR_COLLIDING_VEHICLES_IDS in collision_data:\n",
    "            return True\n",
    "        \n",
    "        current_time = traci.simulation.getTime()\n",
    "        return current_time > 2000  # Change this threshold as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd2c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network for the Q-function\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_state_params, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_state_params, 12)\n",
    "        self.fc2 = nn.Linear(12, 12)\n",
    "        self.fc3 = nn.Linear(12, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e20726f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RL agent\n",
    "class RLAgent:\n",
    "    def __init__(self, n_state_params, n_actions):\n",
    "        self.n_state_params = n_state_params\n",
    "        self.n_actions = n_actions\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 0.05  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = DQN(n_state_params, n_actions)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        state = torch.FloatTensor(state)\n",
    "        q_values = self.model(state)\n",
    "        return np.argmax(q_values.detach().numpy())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model(torch.FloatTensor(next_state)).detach().numpy())\n",
    "            target_f = self.model(torch.FloatTensor(state)).detach().numpy()\n",
    "            # Check if action index is valid\n",
    "            if 0 <= action < self.n_actions:\n",
    "                target_f[action] = target\n",
    "            else:\n",
    "                print(f\"Invalid action: {action}\")\n",
    "\n",
    "            # Convert back to tensor for loss calculation\n",
    "            target_f_tensor = torch.FloatTensor(target_f)\n",
    "            self.model.zero_grad()\n",
    "            loss = self.criterion(target_f_tensor, self.model(torch.FloatTensor(state)))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f581da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation interaction loop\n",
    "def run_simulation(agent, env, num_episodes, batch_size):\n",
    "    for e in range(num_episodes):\n",
    "        state = env.reset_sumo_environment(environment)  # Reset the SUMO environment and get the initial state\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step_in_sumo(action)  # Step through the SUMO simulation\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"Episode: {e+1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "        agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cecb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inputs: 6\n",
      "actions: 4\n",
      "Episode: 1/200, Total Reward: 632431.6158961043\n",
      "Episode: 2/200, Total Reward: 283709.1796984127\n",
      "Episode: 3/200, Total Reward: 1303823.1438771791\n",
      "Episode: 4/200, Total Reward: 5887463.887859379\n",
      "Episode: 5/200, Total Reward: 1473332.609119552\n",
      "Episode: 6/200, Total Reward: 501558.74091417366\n",
      "Episode: 7/200, Total Reward: 1595344.3545406556\n",
      "Episode: 8/200, Total Reward: 3607252.7686544554\n",
      "Episode: 9/200, Total Reward: 2275544.8676035777\n",
      "Episode: 10/200, Total Reward: 4547275.740211551\n",
      "Episode: 11/200, Total Reward: 356632.7821713395\n",
      "Episode: 12/200, Total Reward: 199521.71456426947\n",
      "Episode: 13/200, Total Reward: 2101820.7660489287\n",
      "Episode: 14/200, Total Reward: 372284.5660147457\n",
      "Episode: 15/200, Total Reward: 618447.7229080015\n",
      "Episode: 16/200, Total Reward: 184044.0538173444\n",
      "Episode: 17/200, Total Reward: 1478913.8149463637\n",
      "Episode: 18/200, Total Reward: 11260541.840186104\n",
      "Episode: 19/200, Total Reward: 18345495.49835083\n",
      "Episode: 20/200, Total Reward: 2305926.9497711402\n",
      "Episode: 21/200, Total Reward: 222445.49530293283\n",
      "Episode: 22/200, Total Reward: 5449680.580637243\n",
      "Episode: 23/200, Total Reward: 37129852.19408663\n",
      "Episode: 24/200, Total Reward: 2303596.1787033533\n",
      "Episode: 25/200, Total Reward: 477148.1421457738\n",
      "Episode: 26/200, Total Reward: 961399.4570424411\n",
      "Episode: 27/200, Total Reward: 1475331.1248244834\n",
      "Episode: 28/200, Total Reward: 860594.7575375965\n",
      "Episode: 29/200, Total Reward: 604621.0083450315\n",
      "Episode: 30/200, Total Reward: 2434246.520995722\n",
      "Episode: 31/200, Total Reward: 4167762.361551435\n",
      "Episode: 32/200, Total Reward: 1599690.717009228\n",
      "Episode: 33/200, Total Reward: 808863.7591979152\n",
      "Episode: 34/200, Total Reward: 1792793.8578771714\n",
      "Episode: 35/200, Total Reward: 5825524.645611959\n",
      "Episode: 36/200, Total Reward: 810564.1233386203\n",
      "Episode: 37/200, Total Reward: 17180585.83921773\n",
      "Episode: 38/200, Total Reward: 832696.6182847073\n",
      "Episode: 39/200, Total Reward: 1907768.5904062798\n",
      "Episode: 40/200, Total Reward: 1330285.8891906575\n",
      "Episode: 41/200, Total Reward: 17215580.13889572\n",
      "Episode: 42/200, Total Reward: 3367246.8220863603\n",
      "Episode: 43/200, Total Reward: 3564232.421647324\n",
      "Episode: 44/200, Total Reward: 2074118.847528955\n",
      "Episode: 45/200, Total Reward: 666240.4475113753\n",
      "Episode: 46/200, Total Reward: 18186147.537286192\n",
      "Episode: 47/200, Total Reward: 488077.75550516625\n",
      "Episode: 48/200, Total Reward: 741103.3594634013\n",
      "Episode: 49/200, Total Reward: 15732782.62861646\n",
      "Episode: 50/200, Total Reward: 414166.3869652479\n",
      "Episode: 51/200, Total Reward: 15944471.502995962\n",
      "Episode: 52/200, Total Reward: 6015210.400888569\n",
      "Episode: 53/200, Total Reward: 1596613.7888025173\n",
      "Episode: 54/200, Total Reward: 348475.6275166943\n",
      "Episode: 55/200, Total Reward: 3147375.9385376438\n",
      "Episode: 56/200, Total Reward: 6508071.42372697\n",
      "Episode: 57/200, Total Reward: 1242831.1962620649\n",
      "Episode: 58/200, Total Reward: 3561362.8759250273\n",
      "Episode: 59/200, Total Reward: 7773588.4918033015\n",
      "Episode: 60/200, Total Reward: 8270278.539351003\n",
      "Episode: 61/200, Total Reward: 778709.320635212\n",
      "Episode: 62/200, Total Reward: 50178260.50722879\n",
      "Episode: 63/200, Total Reward: 6084703.811892261\n",
      "Episode: 64/200, Total Reward: 13554336.270522587\n",
      "Episode: 65/200, Total Reward: 1505869.8896077871\n",
      "Episode: 66/200, Total Reward: 9750652.568799444\n",
      "Episode: 67/200, Total Reward: 3836912.0012350064\n",
      "Episode: 68/200, Total Reward: 842765.8995706259\n",
      "Episode: 69/200, Total Reward: 42247369.87112978\n",
      "Episode: 70/200, Total Reward: 167331356.05105397\n",
      "Episode: 71/200, Total Reward: 4122733.033200319\n",
      "Episode: 72/200, Total Reward: 11383235.9360956\n",
      "Episode: 73/200, Total Reward: 15584402.368420979\n",
      "Episode: 74/200, Total Reward: 40344625.26836821\n",
      "Episode: 75/200, Total Reward: 4935271.0392950075\n",
      "Episode: 76/200, Total Reward: 10925120.263952227\n"
     ]
    }
   ],
   "source": [
    "# number of state parameters: parameter for each lane controlled by the traffic light, giving the total delay\n",
    "env = Environment()\n",
    "n_state_params = len(env.get_state())\n",
    "print(\"Number of inputs:\", n_state_params)\n",
    "# Get the full phase program for the traffic light\n",
    "program = traci.trafficlight.getAllProgramLogics(trafficlight_id)[0]\n",
    "\n",
    "# Get the number of phases\n",
    "n_actions = int(len(program.phases) / 2)\n",
    "print(\"actions:\", n_actions)\n",
    "\n",
    "agent = RLAgent(n_state_params, n_actions)\n",
    "run_simulation(agent, env, num_episodes=200, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
