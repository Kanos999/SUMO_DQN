{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "d4c72361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to TraCI\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import traci\n",
    "import sumolib\n",
    "import time\n",
    "\n",
    "sumobin=sumolib.checkBinary('sumo')\n",
    "traci.start([sumobin,'-c','c:\\\\users\\\\kaneh\\\\Documents\\\\COMP9444\\\\SUMO_DQN\\\\environments\\\\cross.sumocfg'])   \n",
    "print(\"Connected to TraCI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac27dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = \"environments/cross.sumocfg\"\n",
    "\n",
    "# Function to reset the SUMO environment\n",
    "def reset_sumo_environment(config_file):\n",
    "    # conn = traci.getConnection('default')\n",
    "    \n",
    "    # Get initial state information (modify this based on your state representation)\n",
    "    state = get_state()\n",
    "\n",
    "    # print(state)\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Function to step through the SUMO simulation\n",
    "def step_in_sumo(action):\n",
    "    # Apply the action (this example assumes action affects traffic lights or vehicle behavior)\n",
    "    apply_action(action)\n",
    "    \n",
    "    # Step the SUMO simulation forward\n",
    "    conn = traci.getConnection('default')\n",
    "    conn.simulationStep()\n",
    "    \n",
    "    # Get the new state after taking the action\n",
    "    next_state = get_state()\n",
    "    \n",
    "    # Calculate the reward (implement your reward logic here)\n",
    "    reward = calculate_reward()\n",
    "    \n",
    "    # Check if the episode is done (implement your termination logic here)\n",
    "    done = check_done_condition()\n",
    "    \n",
    "    return next_state, reward, done\n",
    "\n",
    "# Function to get the current state (modify this based on what information you need)\n",
    "def get_state():\n",
    "    # Example: returning the number of vehicles on the road\n",
    "    num_vehicles = traci.vehicle.getIDCount()\n",
    "    # Add more features as needed (e.g., speeds, positions, etc.)\n",
    "    return np.array([num_vehicles])  # Adjust shape as necessary\n",
    "\n",
    "# Function to apply the action (modify based on your action space)\n",
    "def apply_action(action):\n",
    "    # Example action handling\n",
    "    # Assuming actions control traffic lights (0: Red, 1: Green, etc.)\n",
    "    traffic_light_ids = traci.trafficlight.getIDList()\n",
    "    try:\n",
    "        for tl_id in traffic_light_ids:\n",
    "            if action == 0:\n",
    "                traci.trafficlight.setRedYellowGreenState(tl_id, \"r\")\n",
    "            elif action == 1:\n",
    "                traci.trafficlight.setRedYellowGreenState(tl_id, \"G\")\n",
    "    except traci.exceptions.FatalTraCIError as e:\n",
    "        print(\"TraCI error:\", e)\n",
    "        traci.close()\n",
    "        return\n",
    "\n",
    "# Function to calculate the reward (implement your logic)\n",
    "def calculate_reward():\n",
    "    # Example reward based on average speed of vehicles\n",
    "    speeds = [traci.vehicle.getSpeed(veh_id) for veh_id in traci.vehicle.getIDList()]\n",
    "    return np.mean(speeds)  # Modify reward logic as needed\n",
    "\n",
    "# Function to sum the delay of all vehicles affected by the traffic light\n",
    "def get_delay(tls_id):\n",
    "    delays = [get_lane_delay(lane_id) for lane_id in traci.trafficlight.getControlledLanes(tls_id)]\n",
    "    return sum(delays)\n",
    "\n",
    "def get_lane_delay(lane_id):\n",
    "    max_s = traci.lane.getMaxSpeed(lane_id)\n",
    "    avg_s = traci.lane.getLastStepMeanSpeed(lane_id)\n",
    "    num_veh = traci.lane.getLastStepVehicleNumber(lane_id)\n",
    "    return num_veh * (1 - avg_s / max_s)\n",
    "\n",
    "# Function to get the sum of waiting times of all vehicles currently stopped at the traffic light\n",
    "def get_waiting_time(tls_id):\n",
    "    waiting_times = [get_lane_waiting_time(lane_id) for lane_id in traci.trafficlight.getControlledLanes(tls_id)]\n",
    "    return sum(waiting_times)\n",
    "\n",
    "def get_lane_waiting_time(lane_id):\n",
    "    waiting_times = [traci.vehicle.getWaitingTime(veh_id) for veh_id in traci.lane.getLastStepVehicleIDs(lane_id)]\n",
    "    return sum(waiting_times)\n",
    "\n",
    "# Function that returns the number of emergency stops (acceleration < -4.5m/s^2) caused by the traffic light\n",
    "def num_emergency_stops(tls_id):\n",
    "    emergency_stops = [get_lane_emergency_stops(lane_id) for lane_id in traci.trafficlight.getControlledLanes(tls_id)]\n",
    "    return sum(emergency_stops)\n",
    "\n",
    "def get_lane_emergency_stops(lane_id):\n",
    "    emergency_stops = [veh_id for veh_id in traci.lane.getLastStepVehicleIDs(lane_id) if traci.vehicle.getAcceleration(veh_id) < -4.5]\n",
    "    return len(emergency_stops)\n",
    "\n",
    "# Function to check if the simulation should terminate\n",
    "def check_done_condition():\n",
    "    # Example condition: terminate if simulation time exceeds a limit\n",
    "    current_time = traci.simulation.getTime()\n",
    "    return current_time > 1000  # Change this threshold as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "5dbd2c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network for the Q-function\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "e20726f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RL agent\n",
    "class RLAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state)\n",
    "        q_values = self.model(state)\n",
    "        return np.argmax(q_values.detach().numpy())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model(torch.FloatTensor(next_state)).detach().numpy())\n",
    "            target_f = self.model(torch.FloatTensor(state))\n",
    "            # Check if action index is valid\n",
    "            if 0 <= action < self.action_size:\n",
    "                target_f[action] = target\n",
    "            else:\n",
    "                print(f\"Invalid action: {action}\")\n",
    "\n",
    "            # Convert back to tensor for loss calculation\n",
    "            target_f_tensor = torch.FloatTensor(target_f)\n",
    "            self.model.zero_grad()\n",
    "            loss = self.criterion(target_f_tensor, self.model(torch.FloatTensor(state)))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "f581da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation interaction loop\n",
    "def run_simulation(agent, num_episodes, batch_size):\n",
    "    for e in range(num_episodes):\n",
    "        state = reset_sumo_environment(environment)  # Reset the SUMO environment and get the initial state\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = step_in_sumo(action)  # Step through the SUMO simulation\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"Episode: {e+1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "        agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "55cecb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TraCI error: Connection closed by SUMO.\n"
     ]
    },
    {
     "ename": "TraCIException",
     "evalue": "Connection 'default' is not known.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTraCIException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-475-f84fbe339452>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maction_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m  \u001b[1;31m# Example action size, adjust based on your simulation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRLAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mrun_simulation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-474-6655037d46bc>\u001b[0m in \u001b[0;36mrun_simulation\u001b[1;34m(agent, num_episodes, batch_size)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep_in_sumo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Step through the SUMO simulation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-471-a2e2e340166a>\u001b[0m in \u001b[0;36mstep_in_sumo\u001b[1;34m(action)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Step the SUMO simulation forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraci\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetConnection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'default'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulationStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\main.py\u001b[0m in \u001b[0;36mgetConnection\u001b[1;34m(label)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[0mThrows\u001b[0m \u001b[0ma\u001b[0m \u001b[0mTraCIException\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mno\u001b[0m \u001b[0msuch\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0mexists\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m     \"\"\"\n\u001b[1;32m--> 286\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(label)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"default\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_connections\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTraCIException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Connection '%s' is not known.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_connections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTraCIException\u001b[0m: Connection 'default' is not known."
     ]
    }
   ],
   "source": [
    "# Sample parameters\n",
    "state_size = 4  # Example state size, adjust based on your simulation\n",
    "action_size = 10  # Example action size, adjust based on your simulation\n",
    "agent = RLAgent(state_size, action_size)\n",
    "run_simulation(agent, num_episodes=1000, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
