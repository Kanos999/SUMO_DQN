{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4c72361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import traci\n",
    "import sumolib\n",
    "import math\n",
    "\n",
    "# Initialization for SUMO environment\n",
    "environment = \"intersection/sumo_config.sumocfg\"\n",
    "phase_lane_control = np.array([\n",
    "    [\"N2TL_0\", \"N2TL_1\", \"N2TL_2\", \"S2TL_0\", \"S2TL_1\", \"S2TL_2\"],\n",
    "    [\"N2TL_3\", \"S2TL_3\"],\n",
    "    [\"W2TL_0\", \"W2TL_1\", \"W2TL_2\", \"E2TL_0\", \"E2TL_1\", \"E2TL_2\"],\n",
    "    [\"W2TL_3\", \"E2TL_3\"]\n",
    "], dtype=object)\n",
    "\n",
    "sumobin = sumolib.checkBinary('sumo-gui')\n",
    "traci.start([sumobin, '-c', environment, '--start'])  \n",
    "\n",
    "traci.simulation.subscribe([traci.constants.VAR_COLLIDING_VEHICLES_IDS])\n",
    "\n",
    "# Subscribe to vehicle accelerations for all vehicles\n",
    "for veh_id in traci.vehicle.getIDList():\n",
    "    traci.vehicle.subscribe(veh_id, traci.constants.VAR_ACCELERATION)\n",
    "\n",
    "trafficlight_id = traci.trafficlight.getIDList()[0]\n",
    "controlled_lanes = traci.trafficlight.getControlledLanes(trafficlight_id)\n",
    "TIME_STEP = 0.8  # Simulation time step in seconds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c6152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def get_avg_waiting():\n",
    "    grouped_avg_waiting = [get_lane_num_waiting(lanes) / len(lanes) for lanes in phase_lane_control]\n",
    "    return grouped_avg_waiting\n",
    "\n",
    "def get_lane_num_waiting(lanes):\n",
    "    sum = 0\n",
    "    for lane_id in lanes:\n",
    "        sum += traci.lane.getLastStepHaltingNumber(lane_id)\n",
    "    return sum\n",
    "\n",
    "def get_waiting_ids(lanes):\n",
    "    ids = []\n",
    "    for lane_id in lanes:\n",
    "        ids.extend([veh_id for veh_id in traci.lane.getLastStepVehicleIDs(lane_id) if traci.vehicle.getSpeed(veh_id) < 0.1])\n",
    "    return np.array(ids)\n",
    "\n",
    "def pct_served(waiting_ids):\n",
    "    if len(waiting_ids) == 0:\n",
    "        return 0\n",
    "    still_loaded = [veh_id for veh_id in waiting_ids if veh_id in traci.vehicle.getLoadedIDList()]\n",
    "    num_waiting_served = len([veh_id for veh_id in still_loaded if traci.vehicle.getSpeed(veh_id) > 0.5])\n",
    "    num_waiting_served += len(waiting_ids) - len(still_loaded)\n",
    "    return num_waiting_served / len(waiting_ids)\n",
    "    \n",
    "def get_total_waiting_time():\n",
    "    vehicles = traci.vehicle.getIDList()\n",
    "    waiting_times = [traci.vehicle.getWaitingTime(vehicle) for vehicle in vehicles]\n",
    "    return sum(waiting_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7535ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.prev_action = traci.trafficlight.getPhase(trafficlight_id)\n",
    "        self.yellow_duration = 3\n",
    "        self.green_duration = 25\n",
    "        self.static_action = 0\n",
    "        self.waiting_ids = []\n",
    "        self.pct_served = 0\n",
    "\n",
    "    def reset_sumo_environment(self, environment):\n",
    "        traci.load(['-c', environment, '--start', '--step-length', TIME_STEP])\n",
    "        traci.trafficlight.setProgram(trafficlight_id, '0')\n",
    "        self.waiting_ids = []\n",
    "        self.pct_served = 0\n",
    "        state = self.get_state()\n",
    "        return state\n",
    "\n",
    "    def step_in_sumo(self, action):\n",
    "        self.apply_action(action)\n",
    "        traci.simulationStep()\n",
    "        next_state = self.get_state()\n",
    "        reward = self.calculate_reward()\n",
    "        done = self.check_done_condition()\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def get_state(self):\n",
    "        state = get_avg_waiting()\n",
    "        state.append(self.pct_served)\n",
    "        state.append(self.prev_action)\n",
    "        return np.array(state)\n",
    "\n",
    "    def apply_action(self, action):\n",
    "        if action == self.prev_action:\n",
    "            self.static_action = 1\n",
    "            return\n",
    "        self.simulate_phase(2 * self.prev_action + 1, self.yellow_duration)\n",
    "        self.pct_served = pct_served(self.waiting_ids)\n",
    "        self.waiting_ids = get_waiting_ids(phase_lane_control[action])\n",
    "        self.simulate_phase(2 * action, self.green_duration)\n",
    "        self.prev_action = action\n",
    "\n",
    "    def simulate_phase(self, action, duration):\n",
    "        traci.trafficlight.setPhase(trafficlight_id, action)\n",
    "        steps = 0\n",
    "        while steps < duration / TIME_STEP:\n",
    "            traci.simulationStep()\n",
    "            steps += 1\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        reward = self.static_action + math.exp(4 * self.pct_served) - math.exp(0.2 * sum(get_avg_waiting()))\n",
    "        self.static_action = 0\n",
    "        self.pct_served = 0\n",
    "        return reward\n",
    "\n",
    "    def check_done_condition(self):\n",
    "        collision_data = traci.simulation.getSubscriptionResults()\n",
    "        if collision_data and traci.constants.VAR_COLLIDING_VEHICLES_IDS in collision_data:\n",
    "            print('takkar BC')\n",
    "            return True\n",
    "        current_time = traci.simulation.getTime()\n",
    "        return current_time > 3300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5259cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Double DQN agent\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_state_params, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_state_params, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DoubleDQNAgent:\n",
    "    def __init__(self, n_state_params, n_actions):\n",
    "        self.n_state_params = n_state_params\n",
    "        self.n_actions = n_actions\n",
    "        self.memory = deque(maxlen=3300)\n",
    "        self.gamma = 0.95  # Discount rate\n",
    "        self.epsilon = 0.05  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.999\n",
    "\n",
    "        # Primary and Target Networks\n",
    "        self.model = DQN(n_state_params, n_actions)\n",
    "        self.target_model = DQN(n_state_params, n_actions)\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # Optimizer and loss\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from the primary network to the target network.\"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay memory.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Select an action using an epsilon-greedy policy.\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        state = torch.FloatTensor(state)\n",
    "        q_values = self.model(state)\n",
    "        return np.argmax(q_values.detach().numpy())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"Sample a minibatch from replay memory and update the primary network.\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # Double DQN target calculation\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state_tensor = torch.FloatTensor(next_state)\n",
    "                \n",
    "                # Double DQN: use model to select action, and target_model for Q-value\n",
    "                next_action = np.argmax(self.model(next_state_tensor).detach().numpy())\n",
    "                target_q_value = self.target_model(next_state_tensor).detach().numpy()[next_action]\n",
    "                target += self.gamma * target_q_value\n",
    "\n",
    "            # Prepare for gradient update\n",
    "            target_f = self.model(torch.FloatTensor(state)).detach().numpy()\n",
    "            if 0 <= action < self.n_actions:\n",
    "                target_f[action] = target\n",
    "            else:\n",
    "                print(f\"Invalid action: {action}\")\n",
    "\n",
    "            # Convert back to tensor for loss calculation\n",
    "            target_f_tensor = torch.FloatTensor(target_f)\n",
    "            self.model.zero_grad()\n",
    "            loss = self.criterion(target_f_tensor, self.model(torch.FloatTensor(state)))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon after each replay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def train_target_network(self, update_frequency, episode):\n",
    "        \"\"\"Update target network every 'update_frequency' episodes.\"\"\"\n",
    "        if episode % update_frequency == 0:\n",
    "            self.update_target_network()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df8d7274",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulation interaction loop\n",
    "def run_simulation(agent, env, num_episodes, batch_size, update_frequency=10):\n",
    "    for e in range(num_episodes):\n",
    "        state = env.reset_sumo_environment(environment)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step_in_sumo(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"Episode: {e+1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "        agent.replay(batch_size)\n",
    "        agent.train_target_network(update_frequency, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e3010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/200, Total Reward: 2474.8337734904635\n",
      "Episode: 2/200, Total Reward: 1195.4366010638541\n",
      "Episode: 3/200, Total Reward: -4419.21374449738\n",
      "Episode: 4/200, Total Reward: -47399.04810852992\n",
      "Episode: 5/200, Total Reward: -11639.539085169688\n",
      "Episode: 6/200, Total Reward: -18622.393175228473\n",
      "Episode: 7/200, Total Reward: 2318.4199700932463\n",
      "Episode: 8/200, Total Reward: -1567.0429387653076\n",
      "Episode: 9/200, Total Reward: -5972.931829616328\n",
      "Episode: 10/200, Total Reward: -2958.738113521114\n",
      "Episode: 11/200, Total Reward: 4400.855099147097\n",
      "Episode: 12/200, Total Reward: 3510.566859945309\n",
      "Episode: 13/200, Total Reward: 3008.443402726259\n",
      "Episode: 14/200, Total Reward: 3902.6937350699786\n",
      "Episode: 15/200, Total Reward: 4508.836169436125\n",
      "Episode: 16/200, Total Reward: 4471.553262708733\n",
      "Episode: 17/200, Total Reward: 3948.0644291061008\n",
      "Episode: 18/200, Total Reward: 4570.635917599656\n",
      "Episode: 19/200, Total Reward: 4284.789996151451\n",
      "Episode: 20/200, Total Reward: 3840.748753557468\n",
      "Episode: 21/200, Total Reward: 4052.713562542849\n",
      "Episode: 22/200, Total Reward: 3529.4311240530656\n",
      "Episode: 23/200, Total Reward: -11125.064983495584\n",
      "Episode: 24/200, Total Reward: 2327.6961525282295\n",
      "Episode: 25/200, Total Reward: -1823.174885255394\n",
      "Episode: 26/200, Total Reward: -26441.05337210188\n",
      "Episode: 27/200, Total Reward: 3851.3858745629527\n",
      "Episode: 28/200, Total Reward: 1191.1977208150558\n",
      "Episode: 29/200, Total Reward: 3541.756631241295\n",
      "Episode: 30/200, Total Reward: 4521.705468396044\n",
      "Episode: 31/200, Total Reward: 4733.095533076896\n",
      "Episode: 32/200, Total Reward: 4787.621437415154\n",
      "Episode: 33/200, Total Reward: 4332.656457720792\n",
      "Episode: 34/200, Total Reward: 4428.91743620101\n",
      "Episode: 35/200, Total Reward: 4699.268812799817\n",
      "Episode: 36/200, Total Reward: 4719.843745797904\n",
      "Episode: 37/200, Total Reward: 4539.429686564903\n",
      "Episode: 38/200, Total Reward: 3704.059796696427\n",
      "Episode: 39/200, Total Reward: 305.35432170176927\n",
      "Episode: 40/200, Total Reward: -1438.8542688739374\n",
      "Episode: 41/200, Total Reward: -44436.85814148008\n",
      "Episode: 42/200, Total Reward: 2137.9456012260584\n",
      "Episode: 43/200, Total Reward: 3391.1897220294263\n",
      "Episode: 44/200, Total Reward: 2208.341493161164\n",
      "Episode: 45/200, Total Reward: -3472.892140319894\n",
      "Episode: 46/200, Total Reward: 756.3914562337409\n",
      "Episode: 47/200, Total Reward: 1046.4545009892486\n",
      "Episode: 48/200, Total Reward: 3293.516356544613\n",
      "Episode: 49/200, Total Reward: 4212.8223629137665\n",
      "Episode: 50/200, Total Reward: 2994.9379596410204\n",
      "Episode: 51/200, Total Reward: 3433.3484302974243\n",
      "Episode: 52/200, Total Reward: 3137.5392107786242\n",
      "Episode: 53/200, Total Reward: 4122.395730523407\n",
      "Episode: 54/200, Total Reward: 4052.3475492181583\n",
      "Episode: 55/200, Total Reward: 4107.736784046719\n",
      "Episode: 56/200, Total Reward: 4463.099408638405\n",
      "Episode: 57/200, Total Reward: 4237.525356352424\n",
      "Episode: 58/200, Total Reward: 4657.536884569201\n",
      "Episode: 59/200, Total Reward: 3848.2210725045566\n",
      "Episode: 60/200, Total Reward: 4404.750981904536\n",
      "Episode: 61/200, Total Reward: 4133.172269196775\n",
      "Episode: 62/200, Total Reward: 4463.551910366207\n",
      "Episode: 63/200, Total Reward: 4079.591599360183\n",
      "Episode: 64/200, Total Reward: 3978.731979759096\n",
      "Episode: 65/200, Total Reward: 4421.315218504405\n",
      "Episode: 66/200, Total Reward: 4176.618073384195\n",
      "Episode: 67/200, Total Reward: 4792.536314470756\n",
      "Episode: 68/200, Total Reward: 4651.3957439877895\n",
      "Episode: 69/200, Total Reward: 4451.593668636924\n",
      "Episode: 70/200, Total Reward: 4650.600719493785\n",
      "Episode: 71/200, Total Reward: 4639.013609817357\n",
      "Episode: 72/200, Total Reward: 4662.500538743279\n",
      "Episode: 73/200, Total Reward: 4721.311933146713\n",
      "Episode: 74/200, Total Reward: 3446.366668633805\n",
      "Episode: 75/200, Total Reward: 3887.1541350541697\n",
      "Episode: 76/200, Total Reward: 4225.083848881774\n",
      "Episode: 77/200, Total Reward: 4375.9652049560445\n",
      "Episode: 78/200, Total Reward: 4251.232409646223\n",
      "Episode: 79/200, Total Reward: 4037.1106539349307\n",
      "Episode: 80/200, Total Reward: 4081.834435546367\n",
      "Episode: 81/200, Total Reward: 4352.834692604857\n",
      "Episode: 82/200, Total Reward: 4514.721479330737\n",
      "Episode: 83/200, Total Reward: 4482.603177482847\n",
      "Episode: 84/200, Total Reward: 4680.915869919475\n",
      "Episode: 85/200, Total Reward: 4348.053082814235\n",
      "Episode: 86/200, Total Reward: 4324.532386169675\n",
      "Episode: 87/200, Total Reward: 1184.0049861998461\n",
      "Episode: 88/200, Total Reward: 4001.039015270823\n",
      "Episode: 89/200, Total Reward: 4522.002959143195\n",
      "Episode: 90/200, Total Reward: 4334.926912697379\n",
      "Episode: 91/200, Total Reward: 4243.682107504812\n",
      "Episode: 92/200, Total Reward: 3466.3681474785535\n",
      "Episode: 93/200, Total Reward: 3177.3988588546895\n",
      "Episode: 94/200, Total Reward: 3697.7616742840337\n",
      "Episode: 95/200, Total Reward: 4144.541901714902\n",
      "Episode: 96/200, Total Reward: 4091.5292661386547\n",
      "Episode: 97/200, Total Reward: 4095.9478304382255\n",
      "Episode: 98/200, Total Reward: 4290.539185786644\n",
      "Episode: 99/200, Total Reward: 4265.421864425935\n",
      "Episode: 100/200, Total Reward: 4341.617635507266\n",
      "Episode: 101/200, Total Reward: 4191.840058010466\n",
      "Episode: 102/200, Total Reward: 4226.418066913567\n",
      "Episode: 103/200, Total Reward: 4116.995602626942\n",
      "Episode: 104/200, Total Reward: 4369.337156603224\n",
      "Episode: 105/200, Total Reward: 3428.420748743353\n",
      "Episode: 106/200, Total Reward: -71819.3051326644\n",
      "Episode: 107/200, Total Reward: -3184889.80041188\n",
      "Episode: 108/200, Total Reward: 4793.4720718600975\n",
      "Episode: 109/200, Total Reward: 5162.277965239902\n",
      "Episode: 110/200, Total Reward: 5393.403971596689\n",
      "Episode: 111/200, Total Reward: 5275.456972472624\n",
      "Episode: 112/200, Total Reward: 5372.919366489554\n",
      "Episode: 113/200, Total Reward: 5341.164943727275\n",
      "Episode: 114/200, Total Reward: 5476.8944731350675\n",
      "Episode: 115/200, Total Reward: 5393.448050678069\n",
      "Episode: 116/200, Total Reward: 5355.557719003211\n",
      "Episode: 117/200, Total Reward: -44713.26244340959\n",
      "Episode: 118/200, Total Reward: 5260.372274405781\n",
      "Episode: 119/200, Total Reward: 5327.626899464895\n",
      "Episode: 120/200, Total Reward: 5264.921803971263\n",
      "Episode: 121/200, Total Reward: 5330.49043341377\n",
      "Episode: 122/200, Total Reward: 5351.359658527941\n",
      "Episode: 123/200, Total Reward: 5119.786563619708\n",
      "Episode: 124/200, Total Reward: 5293.010280955775\n",
      "Episode: 125/200, Total Reward: 5139.8634349096565\n",
      "Episode: 126/200, Total Reward: -2514.170272516381\n",
      "Episode: 127/200, Total Reward: -27861.144017926064\n",
      "Episode: 128/200, Total Reward: 2595.237364681026\n",
      "Episode: 129/200, Total Reward: 4981.803118486043\n",
      "Episode: 130/200, Total Reward: 4917.23968733539\n",
      "Episode: 131/200, Total Reward: 4968.603552266779\n",
      "Episode: 132/200, Total Reward: 3474.4763148584066\n",
      "Episode: 133/200, Total Reward: -3214.9429282040473\n",
      "Episode: 134/200, Total Reward: 3952.9794674470663\n",
      "Episode: 135/200, Total Reward: 5259.757761535516\n",
      "Episode: 136/200, Total Reward: 4545.447771799731\n",
      "Episode: 137/200, Total Reward: 4656.310007246636\n",
      "Episode: 138/200, Total Reward: 4756.109566798957\n",
      "Episode: 139/200, Total Reward: 4645.016911231122\n",
      "Episode: 140/200, Total Reward: 5125.242412214007\n",
      "Episode: 141/200, Total Reward: 5418.088654415876\n",
      "Episode: 142/200, Total Reward: 5312.485985841029\n",
      "Episode: 143/200, Total Reward: 4813.094952756065\n",
      "Episode: 144/200, Total Reward: 5274.563870191164\n",
      "Episode: 145/200, Total Reward: 5129.198723011451\n",
      "Episode: 146/200, Total Reward: 4939.541404029277\n",
      "Episode: 147/200, Total Reward: 5040.40472915673\n",
      "Episode: 148/200, Total Reward: 4650.306981385311\n",
      "Episode: 149/200, Total Reward: 5248.569639956838\n",
      "Episode: 150/200, Total Reward: 4376.7987747662\n",
      "Episode: 151/200, Total Reward: 4978.165009244851\n",
      "Episode: 152/200, Total Reward: 5124.838949320944\n",
      "Episode: 153/200, Total Reward: 5033.485455904036\n",
      "Episode: 154/200, Total Reward: 5305.93721929775\n",
      "Episode: 155/200, Total Reward: 5095.153376239174\n",
      "Episode: 156/200, Total Reward: 5136.1486897061795\n",
      "Episode: 157/200, Total Reward: 5286.577980691945\n",
      "Episode: 158/200, Total Reward: 5395.039078774762\n",
      "Episode: 159/200, Total Reward: 5078.017415450329\n",
      "Episode: 160/200, Total Reward: 5316.6660758433645\n",
      "Episode: 161/200, Total Reward: 5005.121692356941\n",
      "Episode: 162/200, Total Reward: 5288.4658604392025\n",
      "Episode: 163/200, Total Reward: 5369.9055615251755\n",
      "Episode: 164/200, Total Reward: 5259.384139422207\n",
      "Episode: 165/200, Total Reward: 5383.599918750168\n",
      "Episode: 166/200, Total Reward: 5217.799734285137\n",
      "Episode: 167/200, Total Reward: 5184.956890441834\n",
      "Episode: 168/200, Total Reward: 5257.3009645684215\n",
      "Episode: 169/200, Total Reward: 5137.667557737839\n",
      "Episode: 170/200, Total Reward: 5177.008767662992\n",
      "Episode: 171/200, Total Reward: 5290.971339598514\n",
      "Episode: 172/200, Total Reward: 5238.563632194497\n",
      "Episode: 173/200, Total Reward: 5241.493070553293\n",
      "Episode: 174/200, Total Reward: 5527.145094693249\n",
      "Episode: 175/200, Total Reward: 5140.695828456742\n",
      "Episode: 176/200, Total Reward: 5121.005853094485\n",
      "Episode: 177/200, Total Reward: 5176.605251241205\n",
      "Episode: 178/200, Total Reward: 5222.948321518249\n",
      "Episode: 179/200, Total Reward: 5149.856270722771\n",
      "Episode: 180/200, Total Reward: 5278.791903585121\n",
      "Episode: 181/200, Total Reward: 5262.933755883043\n",
      "Episode: 182/200, Total Reward: 5011.912524987659\n",
      "Episode: 183/200, Total Reward: 5044.171331693366\n",
      "Episode: 184/200, Total Reward: 5210.082577553844\n",
      "Episode: 185/200, Total Reward: 5236.02559152243\n",
      "Episode: 186/200, Total Reward: 5000.639060873612\n",
      "Episode: 187/200, Total Reward: 4864.217840914082\n",
      "Episode: 188/200, Total Reward: 4904.491942603184\n",
      "Episode: 189/200, Total Reward: 4862.9126832973525\n",
      "Episode: 190/200, Total Reward: 4373.204560814616\n",
      "Episode: 191/200, Total Reward: 3812.6574760057792\n",
      "Episode: 192/200, Total Reward: 2986.4439832040975\n",
      "Episode: 193/200, Total Reward: -3614.972700216611\n",
      "Episode: 194/200, Total Reward: -218047.14303642343\n",
      "Episode: 195/200, Total Reward: -26905.446537120974\n",
      "Episode: 196/200, Total Reward: -34745.479801937894\n",
      "Episode: 197/200, Total Reward: 3876.638830199331\n",
      "Episode: 198/200, Total Reward: 5119.145171975591\n",
      "Episode: 199/200, Total Reward: 5116.44537281959\n",
      "Episode: 200/200, Total Reward: 5364.982459972496\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize environment and agent\n",
    "env = Environment()\n",
    "n_state_params = len(env.get_state())\n",
    "program = traci.trafficlight.getAllProgramLogics(trafficlight_id)[0]\n",
    "n_actions = int(len(program.phases) / 2)\n",
    "agent = DoubleDQNAgent(n_state_params, n_actions)\n",
    "\n",
    "# Run simulation\n",
    "run_simulation(agent, env, num_episodes=200, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
