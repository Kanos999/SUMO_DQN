{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction, motivation, problem statement"
      ],
      "metadata": {
        "id": "I7qvoVyjmKAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly define the problem statemend or purpose of the project\n"
      ],
      "metadata": {
        "id": "DJLQ34fDmPHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data sources or RL task"
      ],
      "metadata": {
        "id": "tqLzmEOUmWuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data sources or reinforcement learning tasks are clearly documented and described"
      ],
      "metadata": {
        "id": "Y0EAYb0xmvUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Analysis of Data or RL tasks"
      ],
      "metadata": {
        "id": "hBCKVwXembpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proide details about the properties, number of classes, pre-processing, challenging aspects, etc of the data.\n",
        "\n",
        "Describe the environment."
      ],
      "metadata": {
        "id": "fkjhq50VmzTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models and/or method"
      ],
      "metadata": {
        "id": "bFjO9fE4mggx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models and methods are judiciously chosen and appropriately applied. If building on previous work, identify the source and clearly delineate which parts are your own work."
      ],
      "metadata": {
        "id": "52vBO7jJm7EL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "tfsRQPvBnF_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline"
      ],
      "metadata": {
        "id": "m5YjplEOnxe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment"
      ],
      "metadata": {
        "id": "CnvDGT4ZfmSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install airsim"
      ],
      "metadata": {
        "id": "1gDOJBkgfdBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc500af-fe75-48d8-c9f5-689c0c67cea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting airsim\n",
            "  Using cached airsim-1.8.1.tar.gz (20 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "7UE1NI5sPtLT",
        "outputId": "767e1d2a-ec96-4165-f000-c93d12b350ec"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'airsim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-de7dbb3767e0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mairsim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'airsim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import airsim\n",
        "\n",
        "import csv\n",
        "import math\n",
        "import pprint\n",
        "import time\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class DroneEnv(object):\n",
        "    \"\"\"Drone environment class using AirSim python API\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.client = airsim.MultirotorClient()\n",
        "        self.client.confirmConnection()\n",
        "        self.client.enableApiControl(True)\n",
        "        self.client.armDisarm(True)\n",
        "\n",
        "        self.pose = self.client.simGetVehiclePose()\n",
        "        self.state = self.client.getMultirotorState().kinematics_estimated.position\n",
        "        print(self.state.x_val, self.state.y_val, self.state.z_val)\n",
        "        self.quad_offset = (0, 0, 0)\n",
        "        initX = 162\n",
        "        initY = -320\n",
        "        initZ = -150\n",
        "\n",
        "        self.start_collision = \"Cube\"\n",
        "        self.next_collision = \"Cube\"\n",
        "        self.cnt_collision = 0\n",
        "        self.collision_change = False\n",
        "\n",
        "        self.client.takeoffAsync().join()\n",
        "        print(\"take off moving positon\")\n",
        "        self.client.moveToPositionAsync(initX, initY, initZ, 5).join()\n",
        "\n",
        "        self.ep = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Step\"\"\"\n",
        "        print(\"doing step\")\n",
        "        self.quad_offset = self.interpret_action(action)\n",
        "        print(\"quad_offset: \", self.quad_offset)\n",
        "        quad_state = self.client.getMultirotorState().kinematics_estimated.position\n",
        "        quad_vel = self.client.getMultirotorState().kinematics_estimated.linear_velocity\n",
        "        self.client.moveByVelocityAsync(\n",
        "            quad_vel.x_val + self.quad_offset[0],\n",
        "            quad_vel.y_val + self.quad_offset[1],\n",
        "            quad_vel.z_val + self.quad_offset[2],\n",
        "            20,\n",
        "        ).join()\n",
        "        time.sleep(0.5)\n",
        "\n",
        "        collision_info = self.client.simGetCollisionInfo()\n",
        "\n",
        "        if self.next_collision != collision_info.object_name:\n",
        "            self.collision_change = True\n",
        "\n",
        "        if collision_info.has_collided:\n",
        "            if self.cnt_collision == 0:\n",
        "                self.start_collision = collision_info.object_name\n",
        "                self.next_collision = collision_info.object_name\n",
        "                self.cnt_collision = 1\n",
        "            else:\n",
        "                self.next_collision = collision_info.object_name\n",
        "\n",
        "        quad_state = self.client.getMultirotorState().kinematics_estimated.position\n",
        "        quad_vel = self.client.getMultirotorState().kinematics_estimated.linear_velocity\n",
        "        print(\n",
        "            \"state x:\",\n",
        "            quad_state.x_val,\n",
        "            \" y: \",\n",
        "            quad_state.y_val,\n",
        "            \" z: \",\n",
        "            quad_state.z_val,\n",
        "        )\n",
        "\n",
        "        result = self.compute_reward(quad_state, quad_vel, collision_info)\n",
        "        state = self.get_obs()\n",
        "        done = self.isDone(result)\n",
        "        return state, result, done\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset to initial state\"\"\"\n",
        "        self.client = airsim.MultirotorClient()\n",
        "        self.client.confirmConnection()\n",
        "        self.client.enableApiControl(True)\n",
        "        self.client.armDisarm(True)\n",
        "\n",
        "        self.pose = self.client.simGetVehiclePose()\n",
        "        self.state = self.client.getMultirotorState().kinematics_estimated.position\n",
        "        print(self.state.x_val, self.state.y_val, self.state.z_val)\n",
        "        self.quad_offset = (0, 0, 0)\n",
        "        initX = 162\n",
        "        initY = -320\n",
        "        initZ = -150\n",
        "\n",
        "        self.start_collision = \"Cube\"\n",
        "        self.next_collision = \"Cube\"\n",
        "        self.cnt_collision = 0\n",
        "        self.collision_change = False\n",
        "\n",
        "        self.client.takeoffAsync().join()\n",
        "        print(\"take off moving positon\")\n",
        "        self.client.moveToPositionAsync(initX, initY, initZ, 5).join()\n",
        "        responses = self.client.simGetImages(\n",
        "            [airsim.ImageRequest(\"1\", airsim.ImageType.Scene, False, False)]\n",
        "        )\n",
        "        obs = self.transform_input(responses)\n",
        "\n",
        "        return obs\n",
        "\n",
        "    def get_obs(self):\n",
        "        \"\"\"Get observation\"\"\"\n",
        "        responses = self.client.simGetImages(\n",
        "            [airsim.ImageRequest(\"1\", airsim.ImageType.Scene, False, False)]\n",
        "        )\n",
        "        obs = self.transform_input(responses)\n",
        "        return obs\n",
        "\n",
        "    def get_distance(self, quad_state):\n",
        "        \"\"\"Get distance between current state and goal state\"\"\"\n",
        "        pts = np.array([-10, 10, -10])\n",
        "        quad_pt = np.array(list((quad_state.x_val, quad_state.y_val, quad_state.z_val)))\n",
        "        dist = np.linalg.norm(quad_pt - pts)\n",
        "        return dist\n",
        "\n",
        "    def compute_reward(self, quad_state, quad_vel, collision_info):\n",
        "        \"\"\"Compute reward\"\"\"\n",
        "        thresh_dist = 7\n",
        "        max_dist = 500\n",
        "        beta = 1\n",
        "\n",
        "        z = -10\n",
        "        if self.ep == 0:\n",
        "            if (\n",
        "                self.collision_change == True\n",
        "                and self.next_collision != self.start_collision\n",
        "            ):\n",
        "                if \"Cube\" in self.next_collision:\n",
        "                    dist = 10000000\n",
        "                    dist = self.get_distance(quad_state)\n",
        "                    reward = 50000\n",
        "                else:\n",
        "                    reward = -100\n",
        "            else:\n",
        "                reward = 0\n",
        "        else:\n",
        "            if self.next_collision != self.start_collision:\n",
        "                if \"Cube\" in self.next_collision:\n",
        "                    dist = 10000000\n",
        "                    dist = self.get_distance(quad_state)\n",
        "                    reward = 50000\n",
        "                else:\n",
        "                    reward = -100\n",
        "            else:\n",
        "                reward = 0\n",
        "        if quad_state.z_val < -280:\n",
        "            reward = -100\n",
        "        print(reward)\n",
        "        return reward\n",
        "\n",
        "\n",
        "    def isDone(self, reward):\n",
        "        \"\"\"Check if episode is done\"\"\"\n",
        "        done = 0\n",
        "        if reward <= -10:\n",
        "            done = 1\n",
        "            self.client.armDisarm(False)\n",
        "            self.client.reset()\n",
        "            self.client.enableApiControl(False)\n",
        "            time.sleep(1)\n",
        "        elif reward > 499:\n",
        "            done = 1\n",
        "            self.client.armDisarm(False)\n",
        "            self.client.reset()\n",
        "            self.client.enableApiControl(False)\n",
        "            time.sleep(1)\n",
        "        return done\n",
        "\n",
        "    def transform_input(self, responses):\n",
        "        \"\"\"Transform input binary array to image\"\"\"\n",
        "        response = responses[0]\n",
        "        img1d = np.fromstring(\n",
        "            response.image_data_uint8, dtype=np.uint8\n",
        "        )\n",
        "        img_rgba = img1d.reshape(\n",
        "            response.height, response.width, 4\n",
        "        )\n",
        "        img2d = np.flipud(img_rgba)\n",
        "\n",
        "        image = Image.fromarray(img2d)\n",
        "        im_final = np.array(image.resize((84, 84)).convert(\"L\"))\n",
        "\n",
        "        return im_final\n",
        "\n",
        "    def interpret_action(self, action):\n",
        "        \"\"\"Interprete action\"\"\"\n",
        "        scaling_factor = 5\n",
        "        if action.item() == 0:\n",
        "            self.quad_offset = (0, 0, 0)\n",
        "        elif action.item() == 1:\n",
        "            self.quad_offset = (scaling_factor, 0, 0)\n",
        "        elif action.item() == 2:\n",
        "            self.quad_offset = (0, scaling_factor, 0)\n",
        "        elif action.item() == 3:\n",
        "            self.quad_offset = (0, 0, scaling_factor)\n",
        "        elif action.item() == 4:\n",
        "            self.quad_offset = (-scaling_factor, 0, 0)\n",
        "        elif action.item() == 5:\n",
        "            self.quad_offset = (0, -scaling_factor, 0)\n",
        "        elif action.item() == 6:\n",
        "            self.quad_offset = (0, 0, -scaling_factor)\n",
        "\n",
        "        return self.quad_offset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent (simple)"
      ],
      "metadata": {
        "id": "3J-rx_xNqJMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import wrappers\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "\n",
        "from env import DroneEnv\n",
        "\n",
        "\n",
        "env = DroneEnv()\n",
        "\n",
        "\n",
        "EPISODES = 50  # number of episodes\n",
        "EPS_START = 0.9  # e-greedy threshold start value\n",
        "EPS_END = 0.05  # e-greedy threshold end value\n",
        "EPS_DECAY = 200  # e-greedy threshold decay\n",
        "GAMMA = 0.8  # Q-learning discount factor\n",
        "LR = 0.001  # NN optimizer learning rate\n",
        "BATCH_SIZE = 1  # Q-learning batch size\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self):\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(84, 21),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(21, 7)\n",
        "        )\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), LR)\n",
        "        self.steps_done = 0\n",
        "\n",
        "    def act(self, state):\n",
        "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * self.steps_done / EPS_DECAY)\n",
        "        self.steps_done += 1\n",
        "        if random.random() > eps_threshold:\n",
        "            action = self.model(state).data.max(1)[1]\n",
        "            action = [action.max(1)[1]]\n",
        "            return torch.LongTensor([action])\n",
        "        else:\n",
        "            action = [random.randrange(0, 7)]\n",
        "            return torch.LongTensor([action])\n",
        "\n",
        "    def memorize(self, state, action, reward, next_state):\n",
        "        self.memory.append((state,\n",
        "                            action,\n",
        "                            torch.FloatTensor([reward]),\n",
        "                            torch.FloatTensor([next_state])))\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Experience Replay\"\"\"\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "        batch = random.sample(self.memory, BATCH_SIZE)\n",
        "        states, actions, rewards, next_states = zip(*batch)\n",
        "        states = torch.cat(states)\n",
        "        actions = torch.cat(actions)\n",
        "        rewards = torch.cat(rewards)\n",
        "        next_states = torch.cat(next_states)\n",
        "\n",
        "        current_q = self.model(states)\n",
        "        max_next_q = self.model(next_states).detach().max(1)[0]\n",
        "        expected_q = rewards + (GAMMA * max_next_q)\n",
        "\n",
        "        loss = F.mse_loss(current_q.squeeze(), expected_q)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "agent = DQNAgent()\n",
        "score_history = []\n",
        "reward_history = []\n",
        "score = 0\n",
        "\n",
        "for e in range(1, EPISODES+1):\n",
        "    state = env.reset()\n",
        "    steps = 0\n",
        "    while True:\n",
        "        state = torch.FloatTensor([state])\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done = env.step(action)\n",
        "\n",
        "        agent.memorize(state, action, reward, next_state)\n",
        "        agent.learn()\n",
        "\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "        score += reward\n",
        "\n",
        "        if done:\n",
        "            print(\"episode:{0}, reward: {1}, score: {2}\".format(e, reward, score))\n",
        "            print(\"----------------------------------------------------\")\n",
        "            score_history.append(steps)\n",
        "            reward_history.append(reward)\n",
        "            f = open('reward.txt', 'a')\n",
        "            f.write(str(reward))\n",
        "            f.close()\n",
        "            f2 = open('score.txt', 'a')\n",
        "            f2.write(str(score))\n",
        "            f2.close()\n",
        "            break"
      ],
      "metadata": {
        "id": "RIKTQc_yqMuw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "22cf148d-4265-4185-e71b-9b34737bac15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'env'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5354d61cdd0f>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDroneEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'env'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent (more complex)"
      ],
      "metadata": {
        "id": "VEQjmc-kf__r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import gym\n",
        "from gym import wrappers\n",
        "from env import DroneEnv\n",
        "\n",
        "env = DroneEnv()\n",
        "\n",
        "\n",
        "# Deep Q-network with arbitrary values\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, in_channels=84, num_actions=7):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 84, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(84, 42, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(42, 21, kernel_size=3, stride=1)\n",
        "        self.fc4 = nn.Linear(336, 168)\n",
        "        self.fc5 = nn.Linear(168, num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
        "        return self.fc5(x)\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        eps_start=0.9,\n",
        "        eps_end=0.05,\n",
        "        eps_decay=200,\n",
        "        gamma=0.8,\n",
        "        learning_rate=0.001,\n",
        "        batch_size=1,\n",
        "    ):\n",
        "        self.eps_start = eps_start\n",
        "        self.eps_end = eps_end\n",
        "        self.eps_decay = eps_decay\n",
        "        self.gamma = gamma\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        dqn = DQN()\n",
        "        self.model = dqn.forward()\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), self.learning_rate)\n",
        "        self.steps_done = 0\n",
        "\n",
        "    def act(self, state):\n",
        "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * math.exp(\n",
        "            -1.0 * self.steps_done / self.eps_decay\n",
        "        )\n",
        "        self.steps_done += 1\n",
        "        if random.random() > eps_threshold:\n",
        "            action = self.model(state).data.max(1)[1]\n",
        "            action = [action.max(1)[1]]\n",
        "            return torch.LongTensor([action])\n",
        "        else:\n",
        "            action = [random.randrange(0, 7)]\n",
        "            return torch.LongTensor([action])\n",
        "\n",
        "    def memorize(self, state, action, reward, next_state):\n",
        "        self.memory.append(\n",
        "            (\n",
        "                state,\n",
        "                action,\n",
        "                torch.FloatTensor([reward]),\n",
        "                torch.FloatTensor([next_state]),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states = zip(*batch)\n",
        "        print(actions)\n",
        "        states = torch.cat(states)\n",
        "        actions = torch.cat(actions)\n",
        "        rewards = torch.cat(rewards)\n",
        "        next_states = torch.cat(next_states)\n",
        "        print(actions)\n",
        "        current_q = self.model(states)\n",
        "        max_next_q = self.model(next_states).detach().max(1)[0]\n",
        "        expected_q = rewards + (GAMMA * max_next_q)\n",
        "\n",
        "        loss = F.mse_loss(current_q.squeeze(), expected_q)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def train(self):\n",
        "        score_history = []\n",
        "        reward_history = []\n",
        "        score = 0\n",
        "\n",
        "        for e in range(1, EPISODES + 1):\n",
        "        state = env.reset()\n",
        "        steps = 0\n",
        "        while True:\n",
        "            state = torch.FloatTensor([state])\n",
        "            action = act(state)\n",
        "            print(action)\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            memorize(state, action, reward, next_state)\n",
        "            learn()\n",
        "\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "            score += reward\n",
        "\n",
        "            if done:\n",
        "                print(\"episode:{0}, reward: {1}, score: {2}\".format(e, reward, score))\n",
        "                print(\"----------------------------------------------------\")\n",
        "                score_history.append(steps)\n",
        "                reward_history.append(reward)\n",
        "                f = open(\"reward.txt\", \"a\")\n",
        "                f.write(str(reward))\n",
        "                f.close()\n",
        "                f2 = open(\"score.txt\", \"a\")\n",
        "                f2.write(str(score))\n",
        "                f2.close()\n",
        "                break"
      ],
      "metadata": {
        "id": "bBj9Fr9sgHi6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "40522e1a-be77-4179-a237-0f6b1815ed74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'for' statement on line 109 (<ipython-input-3-9fc886aed666>, line 110)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-9fc886aed666>\"\u001b[0;36m, line \u001b[0;32m110\u001b[0m\n\u001b[0;31m    state = env.reset()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the program\n",
        "\n"
      ],
      "metadata": {
        "id": "zCLWIXhyhWQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent()\n",
        "agent.train()"
      ],
      "metadata": {
        "id": "moEmb91Bhanv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}