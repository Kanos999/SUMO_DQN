{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7qvoVyjmKAw"
   },
   "source": [
    "# **Introduction, motivation, problem statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJLQ34fDmPHa"
   },
   "source": [
    "Urban traffic congestion is a growing challenge in cities worldwide, leading to increased travel times, fuel consumption, and air pollution. Traditional traffic light systems rely on pre-set timers or simple reactivepolicies, often failing to respond effectively to dynamic traffic conditions. This inefficiency not only frustrates commuters but also leads to environmental and economic costs.\n",
    "\n",
    "The task is to develop a deep reinforcement learning algorithm capable of dynamically controlling traffic lights to optimize traffic flow in real-time. Using SUMO (Simulation of Urban MObility), the RL agent will have a realistic traffic model to interact with, and so must learn to efficiently adapt traffic signal policies based on current traffic conditions, minimizing average waiting times, congestion, and improving overall traffic flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqLzmEOUmWuF"
   },
   "source": [
    "# **Data sources or RL task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0EAYb0xmvUA"
   },
   "source": [
    "Data sources or reinforcement learning tasks are clearly documented and described"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBCKVwXembpB"
   },
   "source": [
    "# **Exploratory Analysis of Data or RL tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkjhq50VmzTV"
   },
   "source": [
    "The SUMO environment offers a very complex framework for simulating traffic in a variety of scenarios ranging from simple intersections to highly expansive urban networks. Using NetSim, we created a simple 8-lane intersection to be the environment all RL agents, including our models as well as the baselines, would be interacting with to create a more relevant comparison when looking at results.\n",
    "\n",
    "The method by which the baseline RL agents controll an intersection, and hence the method we adapted, required the intersection to contain a traffic light program, with numerous light phases for the agent to switch between. So when the agent chooses to active the phase that is already activated, it will simple extend its duration. However, when the agent chooses to activate a new phase, it will first activate the yellow phase corresponding to the last green phase before switching to the new one.\n",
    "\n",
    "For our environment, we created 4 distnict green light phases as well as their accompanying yellow light phases. Hence, any RL agent interacting with our SUMO scenario would learn, through the definable reward state fuctions, which of the light phases should be active at the current time to increase the optimality of the traffic flow.\n",
    "\n",
    "Due to the vast library of SUMO apis available to the agents, it is not just the achitecture of the RL models that distinguishes any 2 agents, but the definition of the state of the environment as well as the reward function which is central to governing how the agent interprets the optimization task. The challenging aspect here is adequately defining a reward function that directly translates to the, rather arbitrary, problem statement of improving traffic flow. For example, when creating an agent for this task, one might try to penalize the number of cars waiting at the intersection but find that the optimal solution found by their agent rapidly flickers the lights so that the cars are contantly in motion but few are making it through the intersection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFjO9fE4mggx"
   },
   "source": [
    "# **Models and Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52vBO7jJm7EL"
   },
   "source": [
    "Models and methods are judiciously chosen and appropriately applied. If building on previous work, identify the source and clearly delineate which parts are your own work.\n",
    "\n",
    "The key differences between our model and the baselines lie in our redefinition of the reward function. To combat the agent finding solutions where the rapid changing of the light phase is prioritized, we chose to focus on rewarding the success of each individual light phase. Before a specific phase is activated, we gather a list of all the vehicles waiting at the corresponding red light, and after the phase has changed once again, we look up those vehicles using SUMO api commands and check how many of them made it through the intersection. This ratio is rewarded exponentially to discourage situations where a vehicle would have to wait several light cycles to make it through the single intersection. Along with this, we still needed to discourage the agent from letting the other lanes from building up a very long queue, so we also included a penalty for this number, which is also penalized exponentially so as to not punish a few vehicles waiting too harshly as well as having a severe punishment when there are too many. We also included a simple reward for when the agent decides to extend the duration of the current phase to allow it to create a strong connection between the state of the environment and which of the 4 outputs is the active one.\n",
    "\n",
    "Along with the reward function, we needed to redefine what information the agent receives upon a state call, which is more simple to understand, just give the agent all the information it would need to make the optimal solution using your reward function. In this case, we gave the agent the current number of cars waiting at the intersection, as well as the current success of the active phase expressed as a percentage, and the id number of the active phase to develop a strong connection and incentive to extend the phase duration over switching to a new one.\n",
    "\n",
    "Tweaking the hyperparameters of the RL model also resulted in different convergence rates and stability over a large number of episodes. The most impactful was adjusting the epsilon value and its decay, which governs the rate at which the network explores a random option over choosing using its policy that is currently in development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5YjplEOnxe_"
   },
   "source": [
    "# **Baselines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.dqn.dqn import DQN\n",
    "\n",
    "\n",
    "if \"SUMO_HOME\" in os.environ:\n",
    "  tools = os.path.join(os.environ[\"SUMO_HOME\"], \"tools\")\n",
    "  sys.path.append(tools)\n",
    "else:\n",
    "  sys.exit(\"Please declare the environment variable 'SUMO_HOME'\")\n",
    "import traci\n",
    "\n",
    "from sumo_rl import SumoEnvironment\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  env = SumoEnvironment(\n",
    "    net_file=\"intersection/environment.net.xml\",\n",
    "    route_file=\"intersection/episode_routes.rou.xml\",\n",
    "    out_csv_name=\"outputs/intersection/dqn\",\n",
    "    single_agent=True,\n",
    "    use_gui=True,\n",
    "    num_seconds=5400,\n",
    "    yellow_time=4,\n",
    "    min_green=5,\n",
    "    max_green=60,\n",
    "  )\n",
    "\n",
    "  model = DQN(\n",
    "    env=env,\n",
    "    policy=\"MlpPolicy\",\n",
    "    learning_rate=1e-3,\n",
    "    learning_starts=0,\n",
    "    buffer_size=50000,\n",
    "    train_freq=1,\n",
    "    target_update_interval=500,\n",
    "    exploration_fraction=0.05,\n",
    "    exploration_final_eps=0.01,\n",
    "    verbose=1,\n",
    "  )\n",
    "  model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.dqn.dqn import DQN\n",
    "\n",
    "\n",
    "if \"SUMO_HOME\" in os.environ:\n",
    "  tools = os.path.join(os.environ[\"SUMO_HOME\"], \"tools\")\n",
    "  sys.path.append(tools)\n",
    "else:\n",
    "  sys.exit(\"Please declare the environment variable 'SUMO_HOME'\")\n",
    "import traci\n",
    "\n",
    "from sumo_rl import SumoEnvironment\n",
    "\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "# Initialize the environment (SumoEnvironment)\n",
    "env = SumoEnvironment(\n",
    "  net_file=r\"intersection\\environment.net.xml\",\n",
    "  route_file=r\"intersection\\episode_routes.rou.xml\",\n",
    "  out_csv_name=r\"outputs\\intersection_DoubleDQN\\dqn\",\n",
    "  single_agent=True,\n",
    "  use_gui=True,\n",
    "  num_seconds=5400,\n",
    "  yellow_time=4,\n",
    "  min_green=5,\n",
    "  max_green=60,\n",
    ")\n",
    "\n",
    "# Define policy_kwargs to enable Double DQN\n",
    "policy_kwargs = dict(\n",
    "  net_arch=[128, 128],  \n",
    ")\n",
    "\n",
    "# Initialize the DQN model with Double DQN enabled via policy_kwargs\n",
    "model = DQN(\n",
    "  \"MlpPolicy\",  # Using a Multi-layer Perceptron policy\n",
    "  env,\n",
    "  learning_rate=1e-3,\n",
    "  learning_starts=0,\n",
    "  buffer_size=50000,\n",
    "  train_freq=1,\n",
    "  target_update_interval=500,\n",
    "  exploration_fraction=0.05,\n",
    "  exploration_final_eps=0.01,\n",
    "  verbose=1,\n",
    "  policy_kwargs=policy_kwargs,  # Pass policy_kwargs for additional configuration\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=100000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Our implementations / Improved models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up connection to SUMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import traci\n",
    "import sumolib\n",
    "import math\n",
    "\n",
    "environment = \"intersection/sumo_config.sumocfg\"\n",
    "phase_lane_control = np.array([\n",
    "        [\"N2TL_0\", \"N2TL_1\", \"N2TL_2\", \"S2TL_0\", \"S2TL_1\", \"S2TL_2\"],\n",
    "        [\"N2TL_3\", \"S2TL_3\"],\n",
    "        [\"W2TL_0\", \"W2TL_1\", \"W2TL_2\", \"E2TL_0\", \"E2TL_1\", \"E2TL_2\"],\n",
    "        [\"W2TL_3\", \"E2TL_3\"]\n",
    "    ], dtype=object)\n",
    "\n",
    "sumobin = sumolib.checkBinary('sumo-gui')\n",
    "\n",
    "traci.start([sumobin, '-c', environment, '--start'])  \n",
    "\n",
    "traci.simulation.subscribe([traci.constants.VAR_COLLIDING_VEHICLES_IDS])\n",
    "\n",
    "# Subscribe to vehicle accelerations for all vehicles\n",
    "for veh_id in traci.vehicle.getIDList():\n",
    "    traci.vehicle.subscribe(veh_id, traci.constants.VAR_ACCELERATION)\n",
    "\n",
    "# for single agent\n",
    "trafficlight_id = traci.trafficlight.getIDList()[0]\n",
    "controlled_lanes = traci.trafficlight.getControlledLanes(trafficlight_id)\n",
    "TIME_STEP = 0.8 # amount of time (in seconds) per step of the simulation, i.e. 0.01 => 10ms per step\n",
    "\n",
    "print(\"Connected to TraCI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMO helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the number of vehicles currently waiting\n",
    "def get_avg_waiting():\n",
    "    # grouped lanes by shared green light phases, record the number of cars waiting divided by the number of lanes\n",
    "    grouped_avg_waiting = [get_lane_num_waiting(lanes) / len(lanes) for lanes in phase_lane_control]\n",
    "    return grouped_avg_waiting\n",
    "\n",
    "# returns the total number of cars waiting in the set of lanes\n",
    "def get_lane_num_waiting(lanes):\n",
    "    sum = 0\n",
    "    for lane_id in lanes:\n",
    "        sum += traci.lane.getLastStepHaltingNumber(lane_id)\n",
    "    return sum\n",
    "\n",
    "# returns a list of vehicle ids that are currently stopped in one of the lanes\n",
    "def get_waiting_ids(lanes):\n",
    "    ids = []\n",
    "    for lane_id in lanes:\n",
    "        ids.extend([veh_id for veh_id in traci.lane.getLastStepVehicleIDs(lane_id) if traci.vehicle.getSpeed(veh_id) < 0.1])\n",
    "    return np.array(ids)\n",
    "\n",
    "def pct_served(waiting_ids):\n",
    "    if len(waiting_ids) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # vehicles that have been served but exited simulation need to be counted a different way\n",
    "    still_loaded = [veh_id for veh_id in waiting_ids if veh_id in traci.vehicle.getLoadedIDList()]\n",
    "    num_waiting_served = len([veh_id for veh_id in still_loaded if traci.vehicle.getSpeed(veh_id) > 0.5])\n",
    "    num_waiting_served += len(waiting_ids) - len(still_loaded)\n",
    "\n",
    "    return num_waiting_served / len(waiting_ids)\n",
    "    \n",
    "def get_total_waiting_time():\n",
    "    vehicles = traci.vehicle.getIDList()\n",
    "    waiting_times = [traci.vehicle.getWaitingTime(vehicle) for vehicle in vehicles]\n",
    "    return sum(waiting_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.prev_action = traci.trafficlight.getPhase(trafficlight_id)\n",
    "        self.yellow_duration = 3 # duration of yellow phases in seconds between actions\n",
    "        self.green_duration = 5 # minimum amount of time the green phases are on for\n",
    "\n",
    "        self.static_action = 0 # adds reward for not changing the phase, prevents flickering\n",
    "        self.waiting_ids = [] # list of vehicle ids that were waiting in one of the lanes now greenlit in the current phase\n",
    "        self.pct_served = 0 # percentage of cars waiting at the relevant lanes that made it through on the last light cycle\n",
    "\n",
    "\n",
    "    # Function to reset the SUMO environment\n",
    "    def reset_sumo_environment(self, environment):\n",
    "        # reload the simulation\n",
    "        traci.load(['-c', environment, '--start', '--step-length', TIME_STEP])\n",
    "        traci.trafficlight.setProgram(trafficlight_id, '0')\n",
    "        \n",
    "        # reset some variables\n",
    "        self.waiting_ids = []\n",
    "        self.pct_served = 0\n",
    "        state = self.get_state()\n",
    "\n",
    "        return state\n",
    "\n",
    "\n",
    "    # Function to step through the SUMO simulation\n",
    "    def step_in_sumo(self, action):\n",
    "        # Apply the action\n",
    "        self.apply_action(action)\n",
    "        \n",
    "        # Step the SUMO simulation forward\n",
    "        traci.simulationStep()\n",
    "        \n",
    "        # Get the new state after taking the action\n",
    "        next_state = self.get_state()\n",
    "        \n",
    "        # Calculate the reward with the specified tls_id\n",
    "        reward = self.calculate_reward()\n",
    "        \n",
    "        # Check if the episode is done\n",
    "        done = self.check_done_condition()\n",
    "        \n",
    "        return next_state, reward, done\n",
    "\n",
    "\n",
    "    # Function to get the current state (modify this based on what information you need)\n",
    "    def get_state(self):\n",
    "        # number of cars in the lanes each phase of the traffic light controls\n",
    "        state = get_avg_waiting()\n",
    "        state.append(self.pct_served) # include the served percent of the current phase\n",
    "        state.append(self.prev_action) # include the current action value\n",
    "        \n",
    "        return np.array(state)\n",
    "\n",
    "\n",
    "    # Function to apply the action (modify based on your action space)\n",
    "    def apply_action(self, action):\n",
    "        if action == self.prev_action:\n",
    "            self.static_action = 1\n",
    "            return\n",
    "        \n",
    "        # simulate the yellow light phase corresponding to the last green phase\n",
    "        self.simulate_phase(2 * self.prev_action + 1, self.yellow_duration)\n",
    "\n",
    "        # get the success parameters of the last light phase\n",
    "        self.pct_served = pct_served(self.waiting_ids)\n",
    "        self.waiting_ids = get_waiting_ids(phase_lane_control[action])\n",
    "        \n",
    "        # change to the new green phase, simulate for the minimum amount of time\n",
    "        self.simulate_phase(2 * action, self.green_duration)\n",
    "        self.prev_action = action\n",
    "\n",
    "\n",
    "    # changes the phase and simulates it for the required amount of time\n",
    "    def simulate_phase(self, action, duration):\n",
    "        traci.trafficlight.setPhase(trafficlight_id, action)\n",
    "        steps = 0\n",
    "        while steps < duration / TIME_STEP:\n",
    "            traci.simulationStep()\n",
    "            steps += 1\n",
    "\n",
    "\n",
    "    # Function to calculate the reward (implement your logic)\n",
    "    def calculate_reward(self):\n",
    "        reward = self.static_action + math.exp(4 * self.pct_served) - math.exp(0.2 * sum(get_avg_waiting()))\n",
    "        \n",
    "        self.static_action = 0\n",
    "        self.pct_served = 0\n",
    "        return reward\n",
    "\n",
    "\n",
    "    # Function to check if the simulation should terminate\n",
    "    def check_done_condition(self):\n",
    "        # Example condition: terminate if simulation time exceeds a limit\n",
    "        collision_data = traci.simulation.getSubscriptionResults()\n",
    "        \n",
    "        # Check for any collisions\n",
    "        if collision_data and traci.constants.VAR_COLLIDING_VEHICLES_IDS in collision_data:\n",
    "            return True\n",
    "        \n",
    "        current_time = traci.simulation.getTime()\n",
    "        return current_time > 2000  # Change this threshold as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network for the Q-function\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_state_params, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_state_params, 12)\n",
    "        self.fc2 = nn.Linear(12, 12)\n",
    "        self.fc3 = nn.Linear(12, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RL agent\n",
    "class RLAgent:\n",
    "    def __init__(self, n_state_params, n_actions):\n",
    "        self.n_state_params = n_state_params\n",
    "        self.n_actions = n_actions\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 0.05  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = DQN(n_state_params, n_actions)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        state = torch.FloatTensor(state)\n",
    "        q_values = self.model(state)\n",
    "        return np.argmax(q_values.detach().numpy())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model(torch.FloatTensor(next_state)).detach().numpy())\n",
    "            target_f = self.model(torch.FloatTensor(state)).detach().numpy()\n",
    "            # Check if action index is valid\n",
    "            if 0 <= action < self.n_actions:\n",
    "                target_f[action] = target\n",
    "            else:\n",
    "                print(f\"Invalid action: {action}\")\n",
    "\n",
    "            # Convert back to tensor for loss calculation\n",
    "            target_f_tensor = torch.FloatTensor(target_f)\n",
    "            self.model.zero_grad()\n",
    "            loss = self.criterion(target_f_tensor, self.model(torch.FloatTensor(state)))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation interaction loop\n",
    "def run_simulation(agent, env, num_episodes, batch_size):\n",
    "    for e in range(num_episodes):\n",
    "        state = env.reset_sumo_environment(environment)  # Reset the SUMO environment and get the initial state\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step_in_sumo(action)  # Step through the SUMO simulation\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"Episode: {e+1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "        agent.replay(batch_size)\n",
    "        \n",
    "        \n",
    "# number of state parameters: parameter for each lane controlled by the traffic light, giving the total delay\n",
    "env = Environment()\n",
    "n_state_params = len(env.get_state())\n",
    "print(\"Number of inputs:\", n_state_params)\n",
    "# Get the full phase program for the traffic light\n",
    "program = traci.trafficlight.getAllProgramLogics(trafficlight_id)[0]\n",
    "\n",
    "# Get the number of phases\n",
    "n_actions = int(len(program.phases) / 2)\n",
    "print(\"actions:\", n_actions)\n",
    "\n",
    "agent = RLAgent(n_state_params, n_actions)\n",
    "run_simulation(agent, env, num_episodes=200, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up connection to SUMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import traci\n",
    "import sumolib\n",
    "import math\n",
    "\n",
    "# Initialization for SUMO environment\n",
    "environment = \"intersection/sumo_config.sumocfg\"\n",
    "phase_lane_control = np.array([\n",
    "    [\"N2TL_0\", \"N2TL_1\", \"N2TL_2\", \"S2TL_0\", \"S2TL_1\", \"S2TL_2\"],\n",
    "    [\"N2TL_3\", \"S2TL_3\"],\n",
    "    [\"W2TL_0\", \"W2TL_1\", \"W2TL_2\", \"E2TL_0\", \"E2TL_1\", \"E2TL_2\"],\n",
    "    [\"W2TL_3\", \"E2TL_3\"]\n",
    "], dtype=object)\n",
    "\n",
    "sumobin = sumolib.checkBinary('sumo-gui')\n",
    "traci.start([sumobin, '-c', environment, '--start'])  \n",
    "\n",
    "traci.simulation.subscribe([traci.constants.VAR_COLLIDING_VEHICLES_IDS])\n",
    "\n",
    "# Subscribe to vehicle accelerations for all vehicles\n",
    "for veh_id in traci.vehicle.getIDList():\n",
    "    traci.vehicle.subscribe(veh_id, traci.constants.VAR_ACCELERATION)\n",
    "\n",
    "trafficlight_id = traci.trafficlight.getIDList()[0]\n",
    "controlled_lanes = traci.trafficlight.getControlledLanes(trafficlight_id)\n",
    "TIME_STEP = 0.8  # Simulation time step in seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sumo Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def get_avg_waiting():\n",
    "    grouped_avg_waiting = [get_lane_num_waiting(lanes) / len(lanes) for lanes in phase_lane_control]\n",
    "    return grouped_avg_waiting\n",
    "\n",
    "def get_lane_num_waiting(lanes):\n",
    "    sum = 0\n",
    "    for lane_id in lanes:\n",
    "        sum += traci.lane.getLastStepHaltingNumber(lane_id)\n",
    "    return sum\n",
    "\n",
    "def get_waiting_ids(lanes):\n",
    "    ids = []\n",
    "    for lane_id in lanes:\n",
    "        ids.extend([veh_id for veh_id in traci.lane.getLastStepVehicleIDs(lane_id) if traci.vehicle.getSpeed(veh_id) < 0.1])\n",
    "    return np.array(ids)\n",
    "\n",
    "def pct_served(waiting_ids):\n",
    "    if len(waiting_ids) == 0:\n",
    "        return 0\n",
    "    still_loaded = [veh_id for veh_id in waiting_ids if veh_id in traci.vehicle.getLoadedIDList()]\n",
    "    num_waiting_served = len([veh_id for veh_id in still_loaded if traci.vehicle.getSpeed(veh_id) > 0.5])\n",
    "    num_waiting_served += len(waiting_ids) - len(still_loaded)\n",
    "    return num_waiting_served / len(waiting_ids)\n",
    "    \n",
    "def get_total_waiting_time():\n",
    "    vehicles = traci.vehicle.getIDList()\n",
    "    waiting_times = [traci.vehicle.getWaitingTime(vehicle) for vehicle in vehicles]\n",
    "    return sum(waiting_times)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.prev_action = traci.trafficlight.getPhase(trafficlight_id)\n",
    "        self.yellow_duration = 3\n",
    "        self.green_duration = 25\n",
    "        self.static_action = 0\n",
    "        self.waiting_ids = []\n",
    "        self.pct_served = 0\n",
    "\n",
    "    def reset_sumo_environment(self, environment):\n",
    "        traci.load(['-c', environment, '--start', '--step-length', TIME_STEP])\n",
    "        traci.trafficlight.setProgram(trafficlight_id, '0')\n",
    "        self.waiting_ids = []\n",
    "        self.pct_served = 0\n",
    "        state = self.get_state()\n",
    "        return state\n",
    "\n",
    "    def step_in_sumo(self, action):\n",
    "        self.apply_action(action)\n",
    "        traci.simulationStep()\n",
    "        next_state = self.get_state()\n",
    "        reward = self.calculate_reward()\n",
    "        done = self.check_done_condition()\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def get_state(self):\n",
    "        state = get_avg_waiting()\n",
    "        state.append(self.pct_served)\n",
    "        state.append(self.prev_action)\n",
    "        return np.array(state)\n",
    "\n",
    "    def apply_action(self, action):\n",
    "        if action == self.prev_action:\n",
    "            self.static_action = 1\n",
    "            return\n",
    "        self.simulate_phase(2 * self.prev_action + 1, self.yellow_duration)\n",
    "        self.pct_served = pct_served(self.waiting_ids)\n",
    "        self.waiting_ids = get_waiting_ids(phase_lane_control[action])\n",
    "        self.simulate_phase(2 * action, self.green_duration)\n",
    "        self.prev_action = action\n",
    "\n",
    "    def simulate_phase(self, action, duration):\n",
    "        traci.trafficlight.setPhase(trafficlight_id, action)\n",
    "        steps = 0\n",
    "        while steps < duration / TIME_STEP:\n",
    "            traci.simulationStep()\n",
    "            steps += 1\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        reward = self.static_action + math.exp(4 * self.pct_served) - math.exp(0.2 * sum(get_avg_waiting()))\n",
    "        self.static_action = 0\n",
    "        self.pct_served = 0\n",
    "        return reward\n",
    "\n",
    "    def check_done_condition(self):\n",
    "        collision_data = traci.simulation.getSubscriptionResults()\n",
    "        if collision_data and traci.constants.VAR_COLLIDING_VEHICLES_IDS in collision_data:\n",
    "            print('takkar BC')\n",
    "            return True\n",
    "        current_time = traci.simulation.getTime()\n",
    "        return current_time > 3300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Double DQN agent\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_state_params, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_state_params, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double DQN Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent:\n",
    "    def __init__(self, n_state_params, n_actions):\n",
    "        self.n_state_params = n_state_params\n",
    "        self.n_actions = n_actions\n",
    "        self.memory = deque(maxlen=3300)\n",
    "        self.gamma = 0.95  # Discount rate\n",
    "        self.epsilon = 0.05  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.999\n",
    "\n",
    "        # Primary and Target Networks\n",
    "        self.model = DQN(n_state_params, n_actions)\n",
    "        self.target_model = DQN(n_state_params, n_actions)\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # Optimizer and loss\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from the primary network to the target network.\"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay memory.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Select an action using an epsilon-greedy policy.\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        state = torch.FloatTensor(state)\n",
    "        q_values = self.model(state)\n",
    "        return np.argmax(q_values.detach().numpy())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"Sample a minibatch from replay memory and update the primary network.\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # Double DQN target calculation\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state_tensor = torch.FloatTensor(next_state)\n",
    "                \n",
    "                # Double DQN: use model to select action, and target_model for Q-value\n",
    "                next_action = np.argmax(self.model(next_state_tensor).detach().numpy())\n",
    "                target_q_value = self.target_model(next_state_tensor).detach().numpy()[next_action]\n",
    "                target += self.gamma * target_q_value\n",
    "\n",
    "            # Prepare for gradient update\n",
    "            target_f = self.model(torch.FloatTensor(state)).detach().numpy()\n",
    "            if 0 <= action < self.n_actions:\n",
    "                target_f[action] = target\n",
    "            else:\n",
    "                print(f\"Invalid action: {action}\")\n",
    "\n",
    "            # Convert back to tensor for loss calculation\n",
    "            target_f_tensor = torch.FloatTensor(target_f)\n",
    "            self.model.zero_grad()\n",
    "            loss = self.criterion(target_f_tensor, self.model(torch.FloatTensor(state)))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon after each replay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def train_target_network(self, update_frequency, episode):\n",
    "        \"\"\"Update target network every 'update_frequency' episodes.\"\"\"\n",
    "        if episode % update_frequency == 0:\n",
    "            self.update_target_network()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation interaction loop\n",
    "def run_simulation(agent, env, num_episodes, batch_size, update_frequency=10):\n",
    "    for e in range(num_episodes):\n",
    "        state = env.reset_sumo_environment(environment)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step_in_sumo(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"Episode: {e+1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "        agent.replay(batch_size)\n",
    "        agent.train_target_network(update_frequency, e)\n",
    "\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = Environment()\n",
    "n_state_params = len(env.get_state())\n",
    "program = traci.trafficlight.getAllProgramLogics(trafficlight_id)[0]\n",
    "n_actions = int(len(program.phases) / 2)\n",
    "agent = DoubleDQNAgent(n_state_params, n_actions)\n",
    "\n",
    "# Run simulation\n",
    "run_simulation(agent, env, num_episodes=200, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfsRQPvBnF_j"
   },
   "source": [
    "# **Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Discussion**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "history_visible": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
