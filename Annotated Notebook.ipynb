{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7qvoVyjmKAw"
   },
   "source": [
    "# **Introduction, motivation, problem statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJLQ34fDmPHa"
   },
   "source": [
    "Urban traffic congestion is a growing challenge in cities worldwide, leading to increased travel times, fuel consumption, and air pollution. Traditional traffic light systems rely on pre-set timers or simple reactivepolicies, often failing to respond effectively to dynamic traffic conditions. This inefficiency not only frustrates commuters but also leads to environmental and economic costs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqLzmEOUmWuF"
   },
   "source": [
    "# **Reinforcement Learning Task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0EAYb0xmvUA"
   },
   "source": [
    "Dataset URL: https://github.com/eclipse-sumo/sumo\n",
    "\n",
    "The task is to develop a deep reinforcement learning algorithm capable of dynamically controlling traffic lights to optimize traffic flow in real-time. Using SUMO (Simulation of Urban MObility), the RL agent will have a realistic traffic model to interact with, and so must learn to efficiently adapt traffic signal policies based on current traffic conditions, minimizing average waiting times, congestion, and improving overall traffic flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBCKVwXembpB"
   },
   "source": [
    "# **Exploratory Analysis of RL task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkjhq50VmzTV"
   },
   "source": [
    "The SUMO environment offers a very complex framework for simulating traffic in a variety of scenarios ranging from simple intersections to highly expansive urban networks. Using NetSim, we created a simple 8-lane intersection to be the environment all RL agents, including our models as well as the baselines, would be interacting with to create a more relevant comparison when looking at results.\n",
    "\n",
    "The method by which the baseline RL agents controll an intersection, and hence the method we adapted, required the intersection to contain a traffic light program, with numerous light phases for the agent to switch between. So when the agent chooses to active the phase that is already activated, it will simple extend its duration. However, when the agent chooses to activate a new phase, it will first activate the yellow phase corresponding to the last green phase before switching to the new one.\n",
    "\n",
    "Due to the vast library of SUMO apis available to the agents, it is not just the achitecture of the RL models that distinguishes any 2 agents, but the definition of the reward function which is central to governing how the agent interprets the optimization task as well as defining the state at any timestep. The challenging aspect here is adequately defining a reward function that translates to the, rather arbitrary, problem statement of improving traffic flow. For example, when creating an agent for this task, one might try to penalize the number of cars waiting at the intersection but find that the optimal solution found by their agent rapidly flickers the lights so that the cars are contantly in motion but few are making it through the intersection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5YjplEOnxe_"
   },
   "source": [
    "# **Baselines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preset Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traci\n",
    "import sumolib\n",
    "\n",
    "environment = \"intersection/sumo_config.sumocfg\"\n",
    "\n",
    "sumobin = sumolib.checkBinary('sumo-gui')\n",
    "traci.start([sumobin, '-c', environment, '--start'])\n",
    "\n",
    "trafficlight_id = traci.trafficlight.getIDList()[0]\n",
    "\n",
    "# Function to reset the SUMO environment\n",
    "def reset_sumo_environment():\n",
    "    # reload the simulation\n",
    "    traci.load(['-c', environment, '--start'])\n",
    "    traci.trafficlight.setProgram(trafficlight_id, '0')\n",
    "\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    reset_sumo_environment()\n",
    "    current_time = 0\n",
    "    \n",
    "    while current_time < 2000:\n",
    "        traci.simulationStep()\n",
    "        current_time = traci.simulation.getTime()\n",
    "\n",
    "    print(f\"Episode: {e+1}/{num_episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.dqn.dqn import DQN\n",
    "\n",
    "\n",
    "if \"SUMO_HOME\" in os.environ:\n",
    "  tools = os.path.join(os.environ[\"SUMO_HOME\"], \"tools\")\n",
    "  sys.path.append(tools)\n",
    "else:\n",
    "  sys.exit(\"Please declare the environment variable 'SUMO_HOME'\")\n",
    "import traci\n",
    "\n",
    "from sumo_rl import SumoEnvironment\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  env = SumoEnvironment(\n",
    "    net_file=\"intersection/environment.net.xml\",\n",
    "    route_file=\"intersection/episode_routes.rou.xml\",\n",
    "    out_csv_name=\"outputs/intersection/dqn\",\n",
    "    single_agent=True,\n",
    "    use_gui=True,\n",
    "    num_seconds=5400,\n",
    "    yellow_time=4,\n",
    "    min_green=5,\n",
    "    max_green=60,\n",
    "  )\n",
    "\n",
    "  model = DQN(\n",
    "    env=env,\n",
    "    policy=\"MlpPolicy\",\n",
    "    learning_rate=1e-3,\n",
    "    learning_starts=0,\n",
    "    buffer_size=50000,\n",
    "    train_freq=1,\n",
    "    target_update_interval=500,\n",
    "    exploration_fraction=0.05,\n",
    "    exploration_final_eps=0.01,\n",
    "    verbose=1,\n",
    "  )\n",
    "  model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.dqn.dqn import DQN\n",
    "\n",
    "\n",
    "if \"SUMO_HOME\" in os.environ:\n",
    "  tools = os.path.join(os.environ[\"SUMO_HOME\"], \"tools\")\n",
    "  sys.path.append(tools)\n",
    "else:\n",
    "  sys.exit(\"Please declare the environment variable 'SUMO_HOME'\")\n",
    "import traci\n",
    "\n",
    "from sumo_rl import SumoEnvironment\n",
    "\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "# Initialize the environment (SumoEnvironment)\n",
    "env = SumoEnvironment(\n",
    "  net_file=r\"intersection\\environment.net.xml\",\n",
    "  route_file=r\"intersection\\episode_routes.rou.xml\",\n",
    "  out_csv_name=r\"outputs\\intersection_DoubleDQN\\dqn\",\n",
    "  single_agent=True,\n",
    "  use_gui=True,\n",
    "  num_seconds=5400,\n",
    "  yellow_time=4,\n",
    "  min_green=5,\n",
    "  max_green=60,\n",
    ")\n",
    "\n",
    "# Define policy_kwargs to enable Double DQN\n",
    "policy_kwargs = dict(\n",
    "  net_arch=[128, 128],  \n",
    ")\n",
    "\n",
    "# Initialize the DQN model with Double DQN enabled via policy_kwargs\n",
    "model = DQN(\n",
    "  \"MlpPolicy\",  # Using a Multi-layer Perceptron policy\n",
    "  env,\n",
    "  learning_rate=1e-3,\n",
    "  learning_starts=0,\n",
    "  buffer_size=50000,\n",
    "  train_freq=1,\n",
    "  target_update_interval=500,\n",
    "  exploration_fraction=0.05,\n",
    "  exploration_final_eps=0.01,\n",
    "  verbose=1,\n",
    "  policy_kwargs=policy_kwargs,  # Pass policy_kwargs for additional configuration\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=100000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Models and Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up connection to SUMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to TraCI\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import traci\n",
    "import sumolib\n",
    "import math\n",
    "\n",
    "environment = \"intersection/sumo_config.sumocfg\"\n",
    "phase_lane_control = np.array([\n",
    "        [\"N2TL_0\", \"N2TL_1\", \"N2TL_2\", \"S2TL_0\", \"S2TL_1\", \"S2TL_2\"],\n",
    "        [\"N2TL_3\", \"S2TL_3\"],\n",
    "        [\"W2TL_0\", \"W2TL_1\", \"W2TL_2\", \"E2TL_0\", \"E2TL_1\", \"E2TL_2\"],\n",
    "        [\"W2TL_3\", \"E2TL_3\"]\n",
    "    ], dtype=object)\n",
    "\n",
    "sumobin = sumolib.checkBinary('sumo-gui')\n",
    "\n",
    "traci.start([sumobin, '-c', environment, '--start'])\n",
    "\n",
    "traci.simulation.subscribe([traci.constants.VAR_COLLIDING_VEHICLES_IDS])\n",
    "\n",
    "# Subscribe to vehicle accelerations for all vehicles\n",
    "for veh_id in traci.vehicle.getIDList():\n",
    "    traci.vehicle.subscribe(veh_id, traci.constants.VAR_ACCELERATION)\n",
    "\n",
    "# for single agent\n",
    "trafficlight_id = traci.trafficlight.getIDList()[0]\n",
    "controlled_lanes = traci.trafficlight.getControlledLanes(trafficlight_id)\n",
    "TIME_STEP = 0.8 # amount of time (in seconds) per step of the simulation, i.e. 0.01 => 10ms per step\n",
    "\n",
    "print(\"Connected to TraCI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMO helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the number of vehicles currently waiting\n",
    "def get_avg_waiting():\n",
    "    # grouped lanes by shared green light phases, record the number of cars waiting divided by the number of lanes\n",
    "    grouped_avg_waiting = [get_lane_num_waiting(lanes) / len(lanes) for lanes in phase_lane_control]\n",
    "    return grouped_avg_waiting\n",
    "\n",
    "# returns the total number of cars waiting in the set of lanes\n",
    "def get_lane_num_waiting(lanes):\n",
    "    sum = 0\n",
    "    for lane_id in lanes:\n",
    "        sum += traci.lane.getLastStepHaltingNumber(lane_id)\n",
    "    return sum\n",
    "\n",
    "# returns a list of vehicle ids that are currently stopped in one of the lanes\n",
    "def get_waiting_ids(lanes):\n",
    "    ids = []\n",
    "    for lane_id in lanes:\n",
    "        ids.extend([veh_id for veh_id in traci.lane.getLastStepVehicleIDs(lane_id) if traci.vehicle.getSpeed(veh_id) < 0.1])\n",
    "    return np.array(ids)\n",
    "\n",
    "def pct_served(waiting_ids):\n",
    "    if len(waiting_ids) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # vehicles that have been served but exited simulation need to be counted a different way\n",
    "    still_loaded = [veh_id for veh_id in waiting_ids if veh_id in traci.vehicle.getLoadedIDList()]\n",
    "    num_waiting_served = len([veh_id for veh_id in still_loaded if traci.vehicle.getSpeed(veh_id) > 0.5])\n",
    "    num_waiting_served += len(waiting_ids) - len(still_loaded)\n",
    "\n",
    "    return num_waiting_served / len(waiting_ids)\n",
    "    \n",
    "def get_total_waiting_time():\n",
    "    vehicles = traci.vehicle.getIDList()\n",
    "    waiting_times = [traci.vehicle.getWaitingTime(vehicle) for vehicle in vehicles]\n",
    "    return sum(waiting_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.prev_action = traci.trafficlight.getPhase(trafficlight_id)\n",
    "        self.yellow_duration = 3 # duration of yellow phases in seconds between actions\n",
    "        self.green_duration = 5 # minimum amount of time the green phases are on for\n",
    "\n",
    "        self.static_action = 0 # adds reward for not changing the phase, prevents flickering\n",
    "        self.waiting_ids = [] # list of vehicle ids that were waiting in one of the lanes now greenlit in the current phase\n",
    "        self.pct_served = 0 # percentage of cars waiting at the relevant lanes that made it through on the last light cycle\n",
    "\n",
    "\n",
    "    # Function to reset the SUMO environment\n",
    "    def reset_sumo_environment(self, environment):\n",
    "        # reload the simulation\n",
    "        traci.load(['-c', environment, '--start', '--step-length', TIME_STEP])\n",
    "        traci.trafficlight.setProgram(trafficlight_id, '0')\n",
    "        \n",
    "        # reset some variables\n",
    "        self.waiting_ids = []\n",
    "        self.pct_served = 0\n",
    "        state = self.get_state()\n",
    "\n",
    "        return state\n",
    "\n",
    "\n",
    "    # Function to step through the SUMO simulation\n",
    "    def step_in_sumo(self, action):\n",
    "        # Apply the action\n",
    "        self.apply_action(action)\n",
    "        \n",
    "        # Step the SUMO simulation forward\n",
    "        traci.simulationStep()\n",
    "        \n",
    "        # Get the new state after taking the action\n",
    "        next_state = self.get_state()\n",
    "        \n",
    "        # Calculate the reward with the specified tls_id\n",
    "        reward = self.calculate_reward()\n",
    "        \n",
    "        # Check if the episode is done\n",
    "        done = self.check_done_condition()\n",
    "        \n",
    "        return next_state, reward, done\n",
    "\n",
    "\n",
    "    # Function to get the current state (modify this based on what information you need)\n",
    "    def get_state(self):\n",
    "        # number of cars in the lanes each phase of the traffic light controls\n",
    "        state = get_avg_waiting()\n",
    "        state.append(self.pct_served) # include the served percent of the current phase\n",
    "        state.append(self.prev_action) # include the current action value\n",
    "        \n",
    "        return np.array(state)\n",
    "\n",
    "\n",
    "    # Function to apply the action (modify based on your action space)\n",
    "    def apply_action(self, action):\n",
    "        if action == self.prev_action:\n",
    "            self.static_action = 1\n",
    "            return\n",
    "        \n",
    "        # simulate the yellow light phase corresponding to the last green phase\n",
    "        self.simulate_phase(2 * self.prev_action + 1, self.yellow_duration)\n",
    "\n",
    "        # get the success parameters of the last light phase\n",
    "        self.pct_served = pct_served(self.waiting_ids)\n",
    "        self.waiting_ids = get_waiting_ids(phase_lane_control[action])\n",
    "        \n",
    "        # change to the new green phase, simulate for the minimum amount of time\n",
    "        self.simulate_phase(2 * action, self.green_duration)\n",
    "        self.prev_action = action\n",
    "\n",
    "\n",
    "    # changes the phase and simulates it for the required amount of time\n",
    "    def simulate_phase(self, action, duration):\n",
    "        traci.trafficlight.setPhase(trafficlight_id, action)\n",
    "        steps = 0\n",
    "        while steps < duration / TIME_STEP:\n",
    "            traci.simulationStep()\n",
    "            steps += 1\n",
    "\n",
    "\n",
    "    # Function to calculate the reward (implement your logic)\n",
    "    def calculate_reward(self):\n",
    "        reward = self.static_action + math.exp(4 * self.pct_served) - math.exp(0.2 * sum(get_avg_waiting()))\n",
    "        \n",
    "        self.static_action = 0\n",
    "        self.pct_served = 0\n",
    "        return reward\n",
    "\n",
    "\n",
    "    # Function to check if the simulation should terminate\n",
    "    def check_done_condition(self):\n",
    "        # Example condition: terminate if simulation time exceeds a limit\n",
    "        collision_data = traci.simulation.getSubscriptionResults()\n",
    "        \n",
    "        # Check for any collisions\n",
    "        if collision_data and traci.constants.VAR_COLLIDING_VEHICLES_IDS in collision_data:\n",
    "            return True\n",
    "        \n",
    "        current_time = traci.simulation.getTime()\n",
    "        return current_time > 2000  # Change this threshold as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network for the Q-function\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_state_params, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_state_params, 12)\n",
    "        self.fc2 = nn.Linear(12, 12)\n",
    "        self.fc3 = nn.Linear(12, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RL agent\n",
    "class RLAgent:\n",
    "    def __init__(self, n_state_params, n_actions):\n",
    "        self.n_state_params = n_state_params\n",
    "        self.n_actions = n_actions\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 0.05  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = DQN(n_state_params, n_actions)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        state = torch.FloatTensor(state)\n",
    "        q_values = self.model(state)\n",
    "        return np.argmax(q_values.detach().numpy())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model(torch.FloatTensor(next_state)).detach().numpy())\n",
    "            target_f = self.model(torch.FloatTensor(state)).detach().numpy()\n",
    "            # Check if action index is valid\n",
    "            if 0 <= action < self.n_actions:\n",
    "                target_f[action] = target\n",
    "            else:\n",
    "                print(f\"Invalid action: {action}\")\n",
    "\n",
    "            # Convert back to tensor for loss calculation\n",
    "            target_f_tensor = torch.FloatTensor(target_f)\n",
    "            self.model.zero_grad()\n",
    "            loss = self.criterion(target_f_tensor, self.model(torch.FloatTensor(state)))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inputs: 6\n",
      "actions: 4\n",
      "Episode: 1/200, Total Reward: -150368.15186837577\n",
      "Episode: 2/200, Total Reward: -13776.5059239758\n",
      "Episode: 3/200, Total Reward: -2061.8097383189574\n",
      "Episode: 4/200, Total Reward: 3835.394697783431\n",
      "Episode: 5/200, Total Reward: -224.66470092972247\n",
      "Episode: 6/200, Total Reward: 3039.657866567434\n",
      "Episode: 7/200, Total Reward: 2409.3052344657235\n",
      "Episode: 8/200, Total Reward: 1329.0604703049016\n",
      "Episode: 9/200, Total Reward: 1274.9859693437254\n",
      "Episode: 10/200, Total Reward: 850.1474864180457\n",
      "Episode: 11/200, Total Reward: 1904.7084240220788\n",
      "Episode: 12/200, Total Reward: 4143.226673564125\n",
      "Episode: 13/200, Total Reward: 3766.861379099308\n",
      "Episode: 14/200, Total Reward: -1737.9875706025498\n",
      "Episode: 15/200, Total Reward: 4423.340685920078\n",
      "Episode: 16/200, Total Reward: 3631.152514960783\n",
      "Episode: 17/200, Total Reward: 4079.7195734691754\n",
      "Episode: 18/200, Total Reward: 3515.013412373573\n",
      "Episode: 19/200, Total Reward: 3424.262214589568\n",
      "Episode: 20/200, Total Reward: -541214.2852729254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_actions)\n\u001b[0;32m     30\u001b[0m agent \u001b[38;5;241m=\u001b[39m RLAgent(n_state_params, n_actions)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mrun_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m, in \u001b[0;36mrun_simulation\u001b[1;34m(agent, env, num_episodes, batch_size)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m      9\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[1;32m---> 10\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_in_sumo\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Step through the SUMO simulation\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n\u001b[0;32m     12\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m, in \u001b[0;36mEnvironment.step_in_sumo\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Calculate the reward with the specified tls_id\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Check if the episode is done\u001b[39;00m\n\u001b[0;32m     41\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_done_condition()\n",
      "Cell \u001b[1;32mIn[4], line 85\u001b[0m, in \u001b[0;36mEnvironment.calculate_reward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_reward\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 85\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_action \u001b[38;5;241m+\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpct_served) \u001b[38;5;241m-\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[43mget_avg_waiting\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpct_served \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m, in \u001b[0;36mget_avg_waiting\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_avg_waiting\u001b[39m():\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# grouped lanes by shared green light phases, record the number of cars waiting divided by the number of lanes\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     grouped_avg_waiting \u001b[38;5;241m=\u001b[39m [\u001b[43mget_lane_num_waiting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanes\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(lanes) \u001b[38;5;28;01mfor\u001b[39;00m lanes \u001b[38;5;129;01min\u001b[39;00m phase_lane_control]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grouped_avg_waiting\n",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m, in \u001b[0;36mget_lane_num_waiting\u001b[1;34m(lanes)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28msum\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lane_id \u001b[38;5;129;01min\u001b[39;00m lanes:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28msum\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtraci\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetLastStepHaltingNumber\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlane_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\_lane.py:270\u001b[0m, in \u001b[0;36mLaneDomain.getLastStepHaltingNumber\u001b[1;34m(self, laneID)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetLastStepHaltingNumber\u001b[39m(\u001b[38;5;28mself\u001b[39m, laneID):\n\u001b[0;32m    265\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"getLastStepHaltingNumber(string) -> integer\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \n\u001b[0;32m    267\u001b[0m \u001b[38;5;124;03m    Returns the total number of halting vehicles for the last time step on the given lane.\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;124;03m    A speed of less than 0.1 m/s is considered a halt.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getUniversal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLAST_STEP_VEHICLE_HALTING_NUMBER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlaneID\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:149\u001b[0m, in \u001b[0;36mDomain._getUniversal\u001b[1;34m(self, varID, objectID, format, *values)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecatedFor:\n\u001b[0;32m    148\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe domain \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated, use \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecatedFor))\n\u001b[1;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retValFunc, varID, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:154\u001b[0m, in \u001b[0;36mDomain._getCmd\u001b[1;34m(self, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FatalTraCIError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot connected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 154\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmdGetID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m r\u001b[38;5;241m.\u001b[39mreadLength()\n\u001b[0;32m    156\u001b[0m response, retVarID \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!BB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:232\u001b[0m, in \u001b[0;36mConnection._sendCmd\u001b[1;34m(self, cmdID, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) \u001b[38;5;241m+\u001b[39m objID\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m packed\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:130\u001b[0m, in \u001b[0;36mConnection._sendExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msending\u001b[39m\u001b[38;5;124m\"\u001b[39m, Storage(length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string)\u001b[38;5;241m.\u001b[39mgetDebugString())\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recvExact()\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Simulation interaction loop\n",
    "def run_simulation(agent, env, num_episodes, batch_size):\n",
    "    for e in range(num_episodes):\n",
    "        state = env.reset_sumo_environment(environment)  # Reset the SUMO environment and get the initial state\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step_in_sumo(action)  # Step through the SUMO simulation\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"Episode: {e+1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "        agent.replay(batch_size)\n",
    "        \n",
    "        \n",
    "# number of state parameters: parameter for each lane controlled by the traffic light, giving the total delay\n",
    "env = Environment()\n",
    "n_state_params = len(env.get_state())\n",
    "print(\"Number of inputs:\", n_state_params)\n",
    "# Get the full phase program for the traffic light\n",
    "program = traci.trafficlight.getAllProgramLogics(trafficlight_id)[0]\n",
    "\n",
    "# Get the number of phases\n",
    "n_actions = int(len(program.phases) / 2)\n",
    "print(\"actions:\", n_actions)\n",
    "\n",
    "agent = RLAgent(n_state_params, n_actions)\n",
    "run_simulation(agent, env, num_episodes=200, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up connection to SUMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import traci\n",
    "import sumolib\n",
    "import math\n",
    "\n",
    "# Initialization for SUMO environment\n",
    "environment = \"intersection/sumo_config.sumocfg\"\n",
    "phase_lane_control = np.array([\n",
    "    [\"N2TL_0\", \"N2TL_1\", \"N2TL_2\", \"S2TL_0\", \"S2TL_1\", \"S2TL_2\"],\n",
    "    [\"N2TL_3\", \"S2TL_3\"],\n",
    "    [\"W2TL_0\", \"W2TL_1\", \"W2TL_2\", \"E2TL_0\", \"E2TL_1\", \"E2TL_2\"],\n",
    "    [\"W2TL_3\", \"E2TL_3\"]\n",
    "], dtype=object)\n",
    "\n",
    "sumobin = sumolib.checkBinary('sumo-gui')\n",
    "traci.start([sumobin, '-c', environment, '--start'])  \n",
    "\n",
    "traci.simulation.subscribe([traci.constants.VAR_COLLIDING_VEHICLES_IDS])\n",
    "\n",
    "# Subscribe to vehicle accelerations for all vehicles\n",
    "for veh_id in traci.vehicle.getIDList():\n",
    "    traci.vehicle.subscribe(veh_id, traci.constants.VAR_ACCELERATION)\n",
    "\n",
    "trafficlight_id = traci.trafficlight.getIDList()[0]\n",
    "controlled_lanes = traci.trafficlight.getControlledLanes(trafficlight_id)\n",
    "TIME_STEP = 0.8  # Simulation time step in seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sumo Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def get_avg_waiting():\n",
    "    grouped_avg_waiting = [get_lane_num_waiting(lanes) / len(lanes) for lanes in phase_lane_control]\n",
    "    return grouped_avg_waiting\n",
    "\n",
    "def get_lane_num_waiting(lanes):\n",
    "    sum = 0\n",
    "    for lane_id in lanes:\n",
    "        sum += traci.lane.getLastStepHaltingNumber(lane_id)\n",
    "    return sum\n",
    "\n",
    "def get_waiting_ids(lanes):\n",
    "    ids = []\n",
    "    for lane_id in lanes:\n",
    "        ids.extend([veh_id for veh_id in traci.lane.getLastStepVehicleIDs(lane_id) if traci.vehicle.getSpeed(veh_id) < 0.1])\n",
    "    return np.array(ids)\n",
    "\n",
    "def pct_served(waiting_ids):\n",
    "    if len(waiting_ids) == 0:\n",
    "        return 0\n",
    "    still_loaded = [veh_id for veh_id in waiting_ids if veh_id in traci.vehicle.getLoadedIDList()]\n",
    "    num_waiting_served = len([veh_id for veh_id in still_loaded if traci.vehicle.getSpeed(veh_id) > 0.5])\n",
    "    num_waiting_served += len(waiting_ids) - len(still_loaded)\n",
    "    return num_waiting_served / len(waiting_ids)\n",
    "    \n",
    "def get_total_waiting_time():\n",
    "    vehicles = traci.vehicle.getIDList()\n",
    "    waiting_times = [traci.vehicle.getWaitingTime(vehicle) for vehicle in vehicles]\n",
    "    return sum(waiting_times)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.prev_action = traci.trafficlight.getPhase(trafficlight_id)\n",
    "        self.yellow_duration = 3\n",
    "        self.green_duration = 25\n",
    "        self.static_action = 0\n",
    "        self.waiting_ids = []\n",
    "        self.pct_served = 0\n",
    "\n",
    "    def reset_sumo_environment(self, environment):\n",
    "        traci.load(['-c', environment, '--start', '--step-length', TIME_STEP])\n",
    "        traci.trafficlight.setProgram(trafficlight_id, '0')\n",
    "        self.waiting_ids = []\n",
    "        self.pct_served = 0\n",
    "        state = self.get_state()\n",
    "        return state\n",
    "\n",
    "    def step_in_sumo(self, action):\n",
    "        self.apply_action(action)\n",
    "        traci.simulationStep()\n",
    "        next_state = self.get_state()\n",
    "        reward = self.calculate_reward()\n",
    "        done = self.check_done_condition()\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def get_state(self):\n",
    "        state = get_avg_waiting()\n",
    "        state.append(self.pct_served)\n",
    "        state.append(self.prev_action)\n",
    "        return np.array(state)\n",
    "\n",
    "    def apply_action(self, action):\n",
    "        if action == self.prev_action:\n",
    "            self.static_action = 1\n",
    "            return\n",
    "        self.simulate_phase(2 * self.prev_action + 1, self.yellow_duration)\n",
    "        self.pct_served = pct_served(self.waiting_ids)\n",
    "        self.waiting_ids = get_waiting_ids(phase_lane_control[action])\n",
    "        self.simulate_phase(2 * action, self.green_duration)\n",
    "        self.prev_action = action\n",
    "\n",
    "    def simulate_phase(self, action, duration):\n",
    "        traci.trafficlight.setPhase(trafficlight_id, action)\n",
    "        steps = 0\n",
    "        while steps < duration / TIME_STEP:\n",
    "            traci.simulationStep()\n",
    "            steps += 1\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        reward = self.static_action + math.exp(4 * self.pct_served) - math.exp(0.2 * sum(get_avg_waiting()))\n",
    "        self.static_action = 0\n",
    "        self.pct_served = 0\n",
    "        return reward\n",
    "\n",
    "    def check_done_condition(self):\n",
    "        collision_data = traci.simulation.getSubscriptionResults()\n",
    "        if collision_data and traci.constants.VAR_COLLIDING_VEHICLES_IDS in collision_data:\n",
    "            print('takkar BC')\n",
    "            return True\n",
    "        current_time = traci.simulation.getTime()\n",
    "        return current_time > 3300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Double DQN agent\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_state_params, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_state_params, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double DQN Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent:\n",
    "    def __init__(self, n_state_params, n_actions):\n",
    "        self.n_state_params = n_state_params\n",
    "        self.n_actions = n_actions\n",
    "        self.memory = deque(maxlen=3300)\n",
    "        self.gamma = 0.95  # Discount rate\n",
    "        self.epsilon = 0.05  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.999\n",
    "\n",
    "        # Primary and Target Networks\n",
    "        self.model = DQN(n_state_params, n_actions)\n",
    "        self.target_model = DQN(n_state_params, n_actions)\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # Optimizer and loss\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from the primary network to the target network.\"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay memory.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Select an action using an epsilon-greedy policy.\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        state = torch.FloatTensor(state)\n",
    "        q_values = self.model(state)\n",
    "        return np.argmax(q_values.detach().numpy())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"Sample a minibatch from replay memory and update the primary network.\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # Double DQN target calculation\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state_tensor = torch.FloatTensor(next_state)\n",
    "                \n",
    "                # Double DQN: use model to select action, and target_model for Q-value\n",
    "                next_action = np.argmax(self.model(next_state_tensor).detach().numpy())\n",
    "                target_q_value = self.target_model(next_state_tensor).detach().numpy()[next_action]\n",
    "                target += self.gamma * target_q_value\n",
    "\n",
    "            # Prepare for gradient update\n",
    "            target_f = self.model(torch.FloatTensor(state)).detach().numpy()\n",
    "            if 0 <= action < self.n_actions:\n",
    "                target_f[action] = target\n",
    "            else:\n",
    "                print(f\"Invalid action: {action}\")\n",
    "\n",
    "            # Convert back to tensor for loss calculation\n",
    "            target_f_tensor = torch.FloatTensor(target_f)\n",
    "            self.model.zero_grad()\n",
    "            loss = self.criterion(target_f_tensor, self.model(torch.FloatTensor(state)))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon after each replay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def train_target_network(self, update_frequency, episode):\n",
    "        \"\"\"Update target network every 'update_frequency' episodes.\"\"\"\n",
    "        if episode % update_frequency == 0:\n",
    "            self.update_target_network()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/200, Total Reward: -57010.95386306469\n",
      "Episode: 2/200, Total Reward: -7554.14254449663\n",
      "Episode: 3/200, Total Reward: -148201.11396121263\n",
      "Episode: 4/200, Total Reward: -9576.780691198876\n",
      "Episode: 5/200, Total Reward: -44917.928764175274\n",
      "Episode: 6/200, Total Reward: -63919.36106463328\n",
      "Episode: 7/200, Total Reward: -29817.006145240757\n",
      "Episode: 8/200, Total Reward: 3244.0908053237304\n",
      "Episode: 9/200, Total Reward: 4321.277429511865\n",
      "Episode: 10/200, Total Reward: 3947.9031082897477\n",
      "Episode: 11/200, Total Reward: 3411.043955194727\n",
      "Episode: 12/200, Total Reward: 4144.071788057313\n",
      "Episode: 13/200, Total Reward: 4044.3482756774424\n",
      "Episode: 14/200, Total Reward: 3299.1847051534937\n",
      "Episode: 15/200, Total Reward: 2840.901768566655\n",
      "Episode: 16/200, Total Reward: -68621.29862786568\n",
      "Episode: 17/200, Total Reward: 749.1485801259033\n",
      "Episode: 18/200, Total Reward: 3889.2306740349027\n",
      "Episode: 19/200, Total Reward: 459.81637499685655\n",
      "Episode: 20/200, Total Reward: 815.5488888228346\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m agent \u001b[38;5;241m=\u001b[39m DoubleDQNAgent(n_state_params, n_actions)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Run simulation\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mrun_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m, in \u001b[0;36mrun_simulation\u001b[1;34m(agent, env, num_episodes, batch_size, update_frequency)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m      9\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[1;32m---> 10\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_in_sumo\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n\u001b[0;32m     12\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mEnvironment.step_in_sumo\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_in_sumo\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_action(action)\n\u001b[1;32m---> 20\u001b[0m     \u001b[43mtraci\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulationStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[0;32m     22\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_reward()\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\main.py:198\u001b[0m, in \u001b[0;36msimulationStep\u001b[1;34m(step)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimulationStep\u001b[39m(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    193\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"simulationStep(float) -> None\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    Make a simulation step and simulate up to the given second in sim time.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m    If the given value is 0 or absent, exactly one step is performed.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m    Values smaller than or equal to the current sim time result in no action.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulationStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:369\u001b[0m, in \u001b[0;36mConnection.simulationStep\u001b[1;34m(self, step)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(step) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[0;32m    368\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI change now handles step as floating point seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 369\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCMD_SIMSTEP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subscriptionResults \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscriptionMapping\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m    371\u001b[0m     subscriptionResults\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:232\u001b[0m, in \u001b[0;36mConnection._sendCmd\u001b[1;34m(self, cmdID, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) \u001b[38;5;241m+\u001b[39m objID\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m packed\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:131\u001b[0m, in \u001b[0;36mConnection._sendExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msending\u001b[39m\u001b[38;5;124m\"\u001b[39m, Storage(length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string)\u001b[38;5;241m.\u001b[39mgetDebugString())\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket\u001b[38;5;241m.\u001b[39msend(length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string)\n\u001b[1;32m--> 131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recvExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceiving\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mgetDebugString())\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:109\u001b[0m, in \u001b[0;36mConnection._recvExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytes\u001b[39m()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m--> 109\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t:\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Simulation interaction loop\n",
    "def run_simulation(agent, env, num_episodes, batch_size, update_frequency=10):\n",
    "    for e in range(num_episodes):\n",
    "        state = env.reset_sumo_environment(environment)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step_in_sumo(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"Episode: {e+1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "        agent.replay(batch_size)\n",
    "        agent.train_target_network(update_frequency, e)\n",
    "\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = Environment()\n",
    "n_state_params = len(env.get_state())\n",
    "program = traci.trafficlight.getAllProgramLogics(trafficlight_id)[0]\n",
    "n_actions = int(len(program.phases) / 2)\n",
    "agent = DoubleDQNAgent(n_state_params, n_actions)\n",
    "\n",
    "# Run simulation\n",
    "run_simulation(agent, env, num_episodes=200, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfsRQPvBnF_j"
   },
   "source": [
    "# **Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Directories where episode CSV files for both sets are stored\n",
    "directory1 = 'outputs/baseline-1'\n",
    "directory2 = 'outputs/baseline-2'\n",
    "directory3 = 'outputs/'\n",
    "\n",
    "# Initialize lists to store the average waiting time per episode for both sets\n",
    "average_waiting_times_1 = []\n",
    "average_waiting_times_2 = []\n",
    "\n",
    "# Function to calculate average waiting times from a directory\n",
    "def calculate_average_waiting_times(directory):\n",
    "    average_waiting_times = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            # Load the CSV file\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Calculate the average waiting time for this episode\n",
    "            avg_waiting_time = data['system_total_waiting_time'].mean()\n",
    "            average_waiting_times.append(avg_waiting_time)\n",
    "    return average_waiting_times\n",
    "\n",
    "# Get average waiting times for both sets\n",
    "average_waiting_times_1 = calculate_average_waiting_times(directory1)\n",
    "average_waiting_times_2 = calculate_average_waiting_times(directory2)\n",
    "\n",
    "\n",
    "data = pd.read_csv('outputs/simulation_results_wait_time.csv')\n",
    "average_waiting_times_3 = data['system_total_waiting_time']\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(average_waiting_times_1, marker='o', linestyle='-', label='DQN')\n",
    "plt.plot(average_waiting_times_2, marker='o', linestyle='-', label='Double DQN')\n",
    "plt.plot(average_waiting_times_3, marker='o', linestyle='-', label='Our model')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average System Total Waiting Time')\n",
    "plt.title('Average System Total Waiting Time Across Episodes')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN\n",
    "\n",
    "![dqn](results/dqn_b.png)\n",
    "\n",
    "Double DQN\n",
    "\n",
    "![ddqn](results/ddqn_b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eps = 0.05](results/dqn.png)\n",
    "\n",
    "![eps = 0.9](results/dqn_0.9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eps = 0.05](results/ddqn.png)\n",
    "\n",
    "![eps = 0.5](results/ddqn_0.5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Discussion**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "history_visible": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
